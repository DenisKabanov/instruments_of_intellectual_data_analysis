{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решить задачу классификации рукописных цифр на датасете mnist https://www.kaggle.com/datasets/hojjatk/mnist-dataset. Правила следующие:\n",
    "- нужно представить решение в виде нейронной сети, написанной на numpy, и обученной с помощью алгоритма градиентного спуска;\n",
    "- нейронная сеть должна состоять из двух линейных слоев, активаций relu и softmax, и mse лосса;\n",
    "- нельзя пользоваться автоградиентом (pytorch, numpy). Градиенты должны считаться вручную по алгоритму обратного распространения ошибки, используя аналитические формулы производных;\n",
    "- решение считается валидным, если оно достигает аккураси больше 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройки/Гиперпараметры/Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # для вывода графиков/картинок\n",
    "import numpy as np # для работы с матрицами\n",
    "\n",
    "import torchvision # для работы с картинками (преобразований)\n",
    "import torch # для создания модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка и обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# преобразования над датасетом\n",
    "transforms = torchvision.transforms.Compose([ # Compose объединяет несколько преобразований вместе, чтобы они выполнялись \"последовательно\"\n",
    "    torchvision.transforms.ToTensor(), # преобразование PIL изображения (или ndarray формата (Height x Width x Channels)) в tensor (типа float со значениями в области [0.0, 1.0], если такая трансформация поддерживается (см описание ToTensor))\n",
    "    torchvision.transforms.Normalize(mean=(0.1307,), std=(0.3081,)) # нормализация каналов (в датасете всего один канал) к указанным среднему значению и стандартному отклонению (цифры подобраны под датасет)\n",
    "])\n",
    "\n",
    "# датасет для обучения\n",
    "data_train = torchvision.datasets.MNIST(root=\"./data\", # путь, откуда брать/куда сохранять датасет\n",
    "                                        train=True, # скачиание обучающей части датасета\n",
    "                                        download=True, # скачивать ли датасет, если его нет в root пути\n",
    "                                        transform=transforms # функция, принимающая на вход PIL Image и преобразовывающая его\n",
    "                                       )\n",
    "\n",
    "# загрузчик данных для обучения\n",
    "train_loader = torch.utils.data.DataLoader(dataset=data_train, # указание датасета для DataLoader\n",
    "                                           batch_size=20, # размер батчка (число сэмплов, что будет возвращать DataLoader за раз) (градиент усредняется по батчу, ускоряется обработка датасета, но слегка замедляется обработка сэмпла)\n",
    "                                           num_workers=5, # число используемых ядер процессора для ускорения обработки данных\n",
    "                                           pin_memory=True # нужно ли заранее аллоцировать память под объект на GPU (лучше так, чем возвращать CUDA tensors при multi-process loading)\n",
    "                                          )\n",
    "\n",
    "# аналогично для тестирования\n",
    "data_test = torchvision.datasets.MNIST(root=\"./data\", # путь, откуда брать/куда сохранять датасет\n",
    "                                       train=False, # скачивание тестовой части датасета\n",
    "                                       download=True, # скачивать ли датасет, если его нет в root пути\n",
    "                                       transform=transforms # функция, принимающая на вход PIL Image и преобразовывающая его\n",
    "                                      )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=data_test, # указание датасета для DataLoader\n",
    "                                          batch_size=20, # размер батчка (число сэмплов, что будет возвращать DataLoader за раз) (градиент усредняется по батчу, ускоряется обработка датасета, но слегка замедляется обработка сэмпла)\n",
    "                                          num_workers=5, # число используемых ядер процессора для ускорения обработки данных\n",
    "                                          pin_memory=True # нужно ли заранее аллоцировать память под объект на GPU (лучше так, чем возвращать CUDA tensors при multi-process loading)\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train # данные о обучающем датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ./data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test # данные о тестовом датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860, -0.1951,\n",
       "          -0.1951, -0.1951,  1.1795,  1.3068,  1.8032, -0.0933,  1.6887,\n",
       "           2.8215,  2.7197,  1.1923, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.0424,  0.0340,  0.7722,  1.5359,  1.7396,  2.7960,\n",
       "           2.7960,  2.7960,  2.7960,  2.7960,  2.4396,  1.7650,  2.7960,\n",
       "           2.6560,  2.0578,  0.3904, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           0.1995,  2.6051,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
       "           2.7960,  2.7960,  2.7960,  2.7706,  0.7595,  0.6195,  0.6195,\n",
       "           0.2886,  0.0722, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.1951,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
       "           2.0960,  1.8923,  2.7197,  2.6433, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242,  0.5940,  1.5614,  0.9377,  2.7960,  2.7960,  2.1851,\n",
       "          -0.2842, -0.4242,  0.1231,  1.5359, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.2460, -0.4115,  1.5359,  2.7960,  0.7213,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242,  1.3450,  2.7960,  1.9942,\n",
       "          -0.3988, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.2842,  1.9942,  2.7960,\n",
       "           0.4668, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0213,  2.6433,\n",
       "           2.4396,  1.6123,  0.9504, -0.4115, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6068,\n",
       "           2.6306,  2.7960,  2.7960,  1.0904, -0.1060, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           0.1486,  1.9432,  2.7960,  2.7960,  1.4850, -0.0806, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.2206,  0.7595,  2.7833,  2.7960,  1.9560, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242,  2.7451,  2.7960,  2.7451,  0.3904,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           0.1613,  1.2305,  1.9051,  2.7960,  2.7960,  2.2105, -0.3988,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0722,  1.4596,\n",
       "           2.4906,  2.7960,  2.7960,  2.7960,  2.7578,  1.8923, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.1187,  1.0268,  2.3887,  2.7960,\n",
       "           2.7960,  2.7960,  2.7960,  2.1342,  0.5686, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.1315,  0.4159,  2.2869,  2.7960,  2.7960,  2.7960,\n",
       "           2.7960,  2.0960,  0.6068, -0.3988, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1951,\n",
       "           1.7523,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.0578,\n",
       "           0.5940, -0.3097, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242,  0.2758,  1.7650,  2.4524,\n",
       "           2.7960,  2.7960,  2.7960,  2.7960,  2.6815,  1.2686, -0.2842,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242,  1.3068,  2.7960,  2.7960,\n",
       "           2.7960,  2.2742,  1.2941,  1.2559, -0.2206, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0][0] # первое \"изображение\" датасета после применения трансформаций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0][1] # класс (target) первого изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data_train[0][0].reshape((28, 28, 1)), cmap='gray') # построение изображения\n",
    "# .reshape((28, 28, 1)), так как shape(1, 28, 28) ~ (Channels, Height, Width) не воспринимается\n",
    "# cmap='gray' так как изображение одноканальное, то есть чёрно-белое\n",
    "plt.show() # вывод изображения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Cross-Entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Cross Entropy Loss(p^{pred}) = \\dfrac{1}{batch\\_size} \\sum_{N=1}^{batch\\_size} \\sum_{C=1}^{classes} -p^{true}_{N,C}*log_e(p^{pred}_{N,C})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\dfrac{dCross Entropy Loss(p^{pred})}{dp^{pred}_{C}} = \\dfrac{1}{batch\\_size} \\sum_{N=1}^{batch\\_size} -\\dfrac{p^{true}_{N,C}}{p^{pred}_{N,C}} + \\dfrac{1-p^{true}_{N,C}}{1-p^{pred}_{N,C}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Без усреднения по батчу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Cross Entropy Loss(p^{pred}_N) = \\sum_{C=1}^{classes} -p^{true}_{N,C}*log_e(p^{pred}_{N,C}),\\ где\\ N\\ -\\ номер\\ батча\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\dfrac{dCross Entropy Loss(p^{pred}_N)}{dp^{pred}_{N,C}} = -\\dfrac{p^{true}_{N,C}}{p^{pred}_{N,C}} + \\dfrac{1-p^{true}_{N,C}}{1-p^{pred}_{N,C}},\\ где\\ N\\ -\\ номер\\ батча\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Конструктор функции для подсчёта CrossEntropyLoss.\n",
    "        \"\"\"\n",
    "        self.loss = None # значение подсчитанного loss\n",
    "        self.grad = None # градиент размера (batch_size, classes)\n",
    "        self.p_pred = None # \n",
    "        self.p_true = None\n",
    "        self.batch_size = 1 # размер батча\n",
    "        self.classes = 1 # число классов\n",
    "\n",
    "    def calc_loss(self, p_pred, p_true) -> np.float64: # результат разниться с torch.nn.CrossEntropyLoss()!\n",
    "        \"\"\"\n",
    "        Функция для подсчёта Cross-Entropy loss с усреднением по батчу.\\n\n",
    "        Parameters:\n",
    "            * p_pred: предсказанные вероятности классов размера (batch_size, classes)\n",
    "            * p_true: реальные вероятности классов размера (batch_size, classes)\\n\n",
    "        Returns:\n",
    "            * np.float64: значение функции потерь\n",
    "        \"\"\"\n",
    "        self.batch_size = p_true.shape[0] # размер батча\n",
    "        self.classes = p_true.shape[1] # число классов\n",
    "        loss = 0.0 # значение loss\n",
    "\n",
    "        # workaround для того, чтобы избавиться от inf и nan\n",
    "        p_pred[p_pred==0.0] = 0.000001 # заменяем полностью нулевые вероятности на очень малые - чтобы логарифм в формуле не давал -inf\n",
    "        p_pred[p_pred==1.0] = 0.999999 # заменяем вероятности в 1 на очень высокие - чтобы в backward в формуле (1-self.p_true[batch][c])/(1-self.p_pred[batch][c])  не получился NaN\n",
    "        p_true[p_true==0.0] = 0.000001 # заменяем полностью нулевые вероятности на очень малые - чтобы логарифм в формуле не давал -inf\n",
    "        p_true[p_true==1.0] = 0.999999 # заменяем вероятности в 1 на очень высокие - чтобы в backward в формуле (1-self.p_true[batch][c])/(1-self.p_pred[batch][c])  не получился NaN\n",
    "\n",
    "        for batch in range(self.batch_size): # идём по числу батчей (внешний цикл)\n",
    "            #========== v1\n",
    "            loss += np.matmul(p_true[batch], np.log(p_pred[batch])) # сумма по классам на определённом батче\n",
    "            #========== v2\n",
    "            # for c in range(classes):\n",
    "            #     loss += p_true[batch][c] * np.log(p_pred[batch][c])\n",
    "            #==========\n",
    "        loss = - loss / self.batch_size # домножаем на -1 и берём среднее по батчам\n",
    "\n",
    "        self.loss = loss # запоминаем подсчитанный loss\n",
    "\n",
    "        #========== v1 (без усреднения градиента по батчам)\n",
    "        self.grad = np.zeros(shape=(self.batch_size, self.classes)) # заготовка под матрицу градиентов\n",
    "        #========== v2 (с усреднением градиента по батчам)\n",
    "        # self.grad = np.zeros(shape=(self.classes)) # заготовка под матрицу градиентов (с усреднением по батчам)\n",
    "        #==========\n",
    "\n",
    "        self.p_pred = p_pred\n",
    "        self.p_true = p_true\n",
    "        return loss # возвращаем посчитанный loss\n",
    "    \n",
    "\n",
    "    def backward(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Функция для подсчёта градиента после Cross-Entropy loss.\\n\n",
    "        Returns:\n",
    "            * np.ndarray: значение градиента, размер (batch_size, classes)\n",
    "        \"\"\"\n",
    "        #========== v1 (если считать, что вход пришёл от softmax, то сразу возвращаем dloss/dsoftmax_input)\n",
    "        # for batch in range(self.batch_size): # идём по номерам батчей\n",
    "        #     #========== v1 с итерированием по классам\n",
    "        #     for c in range(self.classes): # идём по классам\n",
    "        #         self.grad[batch][c] = self.p_pred[batch][c] - self.p_true[batch][c] # считаем градиент при условии, что вход от softmax (см grad.png)\n",
    "        #     #========== v2 без итерирования по классам (работаем с векторами-строками)\n",
    "        #     self.grad[batch] = self.p_pred[batch] - self.p_true[batch] # считаем градиент при условии, что вход от softmax\n",
    "        #     #==========\n",
    "        #========== v1.1 (v1 с усреднением по батчам)\n",
    "        # for batch in range(self.batch_size): # идём по номерам батчей\n",
    "        #     #========== v1 с итерированием по классам\n",
    "        #     # for c in range(self.classes): # идём по классам\n",
    "        #     #     self.grad[c] += self.p_pred[batch][c] - self.p_true[batch][c] # считаем градиент (см grad.png)\n",
    "        #     #========== v2 без итерирования по классам (работаем с векторами-строками)\n",
    "        #     self.grad += self.p_pred[batch] - self.p_true[batch] # считаем градиент\n",
    "        #     #==========\n",
    "        # self.grad = self.grad / self.batch_size # усредняем градиент по числу батчей\n",
    "        #========== v2 (общий случай)\n",
    "        for batch in range(self.batch_size): # идём по номерам батчей\n",
    "            #========== v1 с итерированием по классам\n",
    "            # for c in range(self.classes): # идём по классам\n",
    "            #     self.grad[batch][c] = -self.p_true[batch][c]/self.p_pred[batch][c] + (1-self.p_true[batch][c])/(1-self.p_pred[batch][c])\n",
    "            #========== v2 без итерирования по классам (работаем с векторами-строками)\n",
    "            self.grad[batch] = -(self.p_true[batch]/self.p_pred[batch]) + (1-self.p_true[batch])/(1-self.p_pred[batch]) # считаем градиент от предсказанных вероятностей\n",
    "            #==========\n",
    "        #========== v2.1 (v2 с усреднением по батчам)\n",
    "        # for batch in range(self.batch_size): # идём по номерам батчей\n",
    "        #     #========== v1 с итерированием по классам\n",
    "        #     # for c in range(self.classes): # идём по классам\n",
    "        #     #     self.grad[c] += -(self.p_true[batch][c]/self.p_pred[batch][c]) + (1-self.p_true[batch][c])/(1-self.p_pred[batch][c]) # считаем градиент от предсказанных вероятностей\n",
    "        #     #========== v2 без итерирования по классам (работаем с векторами-строками)\n",
    "        #     self.grad += -(self.p_true[batch]/self.p_pred[batch]) + (1-self.p_true[batch])/(1-self.p_pred[batch]) # считаем градиент от предсказанных вероятностей\n",
    "        #     #==========\n",
    "        # self.grad = self.grad / self.batch_size # усредняем градиент по числу батчей\n",
    "        #==========\n",
    "\n",
    "        return self.grad # возвращаем посчитанный градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9009, dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_pred = np.array([[0.2,0.6,0.2], [1.0,0.0,0.0]])\n",
    "# probs_pred = np.array([[1.0,0.0,0.0], [1.0,0.0,0.0]])\n",
    "probs_pred = torch.tensor(probs_pred)\n",
    "\n",
    "probs_true = np.array([[1.0,0.0,0.0], [1.0,0.0,0.0]])\n",
    "probs_true = torch.tensor(probs_true)\n",
    "\n",
    "loss_torch = torch.nn.CrossEntropyLoss()\n",
    "loss_torch(probs_pred, probs_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8047, dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_custom = CrossEntropyLoss()\n",
    "loss_custom.calc_loss(probs_pred, probs_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.99999375,  2.49999583,  1.24999375],\n",
       "       [ 0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_custom.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "ReLU(x) =  \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      0 & if & x \\leq 0 \\\\\n",
    "      x & if & x > 0 \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\dfrac{dReLU(x)}{dx} =  \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      0 & if & x \\leq 0 \\\\\n",
    "      1 & if & x > 0 \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"pics/ReLU.png\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Конструктор ReLU функции.\n",
    "        \"\"\"\n",
    "        self.prev_input = None # вход с предыдущего слоя\n",
    "\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray: # numpy.array — это просто удобная функция для создания numpy.ndarray\n",
    "        \"\"\"\n",
    "        Функция активации ReLU, если значение в X меньше или равно нулю - оно становится нулём, иначе — остаётся прежним.\\n\n",
    "        Parameters:\n",
    "            * x: данные в виде массива размера (batch_size, features)\\n\n",
    "        Returns:\n",
    "            * np.ndarray: преобразованный x согласно работе функции активации\n",
    "        \"\"\"\n",
    "        self.prev_input = x.copy() # запоминаем вход с предыдущего шага\n",
    "        x[x<0] = 0.0 # заменяем все значения в x, что меньше нуля на ноль\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Функция для обратного прохода градиента.\\n\n",
    "        Parameters:\n",
    "            * grad: градиент, пришедший со следующего шага\\n\n",
    "        Returns:\n",
    "            * np.ndarray: градиент с учётом текущего шага, передающийся назад\n",
    "        \"\"\"\n",
    "        grad[self.prev_input < 0] = 0 # зануляем градиент там, где вход был меньше нуля\n",
    "        # как бы поэлементно умножаем пришедший градиент на матрицу из нулей и единиц (так как это производная ReLU) \n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Softmax(x) = \\dfrac{e^{x_i}}{\\sum_{j}e^{x_j}} = p_i,\\ где\\ i=\\overline{1, classes}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\dfrac{dSoftmax(x)}{dx_i} = \\dfrac{e^{x_i}}{\\sum_{j}e^{x_j}} (1 - \\dfrac{e^{x_i}}{\\sum_{j}e^{x_j}}) = p_i * (1 - p_i),\\ где\\ i=\\overline{1, classes}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Конструктор Softmax функции.\n",
    "        \"\"\"\n",
    "        # print(\"constructed\")\n",
    "        self.prev_input = None # вход с предыдущего слоя\n",
    "        self.prev_out = None # выход со слоя\n",
    "\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Функция Softmax, пересчитывающая для всех элементов массива x значение e^x_i/summ(e^x_i).\\n\n",
    "        Parameters:\n",
    "            * x: данные в виде массива размерности (batch_size, class_count) ~ (размер батча, число классов)\\n\n",
    "        Returns:\n",
    "            * np.ndarray: преобразованный X согласно формуле Softmax\n",
    "        \"\"\"\n",
    "        prob = np.exp(x) # считаем экспоненту от всех элементов X (каждый элемент в X идёт как степень e)\n",
    "\n",
    "        for batch in range(prob.shape[0]): # идём по батчам\n",
    "            prob[batch] = prob[batch] / prob[batch].sum() # каждый элемент (экспоненту) в батче делим на сумму экспонент этого батча\n",
    "        self.prev_out = prob.copy() # запоминаем выход слоя\n",
    "        # print(\"prob\\n\", prob) # DEBUG\n",
    "        return prob # возвращаем результат Softmax\n",
    "    \n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray: # grad = dloss/dinput = dloss/dprob * dprob/dinput, где prob - выход слоя softmax\n",
    "        \"\"\"\n",
    "        Функция для обратного прохода градиента.\\n\n",
    "        Parameters:\n",
    "            * grad: градиент, пришедший со следующего шага\\n\n",
    "        Returns:\n",
    "            * np.ndarray: градиент с учётом текущего шага, передающийся назад\n",
    "        \"\"\"\n",
    "        #========== v1 (если в CrossEntropyLoss учитывается то, что вход от softmax ==> грубо говоря - пропускаем шаг с подсчётом dloss/dprob, а сразу считаем dloss/dinput)\n",
    "        # return grad # просто возвращаем полученный градиент (при условии формулы в CrossEntropyLoss)\n",
    "        #========== v2 (общий случай)\n",
    "        return grad * self.prev_out * (1 - self.prev_out) # домножаем полученный градиент на значение производной (dot product - поэлементное умножение)\n",
    "        #=========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.99833498e-01 6.00333004e-01 1.99833498e-01]\n",
      " [9.99909208e-01 4.53958078e-05 4.53958078e-05]]\n",
      "[[-8.00165502e-01  6.00332004e-01  1.99832498e-01]\n",
      " [-8.97916157e-05  4.43958078e-05  4.43958078e-05]]\n"
     ]
    }
   ],
   "source": [
    "# probs_pred = np.array([[0.2,0.6,0.2], [1.0,0.0,0.0]])\n",
    "# probs_pred = np.array([[1.0,0.0,0.0], [1.0,0.0,0.0]])\n",
    "input = np.array([[2.0,3.1,2.0], [10.0,0.0,0.0]]) # ==> probs_pred ~ [[0.2,0.6,0.2], [1.0,0.0,0.0]]\n",
    "\n",
    "probs_true = np.array([[1.0,0.0,0.0], [1.0,0.0,0.0]])\n",
    "\n",
    "soft = Softmax()\n",
    "res = soft.forward(input)\n",
    "print(res)\n",
    "\n",
    "loss_custom = CrossEntropyLoss()\n",
    "loss_custom.calc_loss(res, np.array([[1.0,0.0,0.0], [1.0,0.0,0.0]]))\n",
    "grad = loss_custom.backward()\n",
    "print(soft.backward(grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Linear(x) = xW + b\n",
    "$$\n",
    "* x - вход размера (batch_size, in_features)\n",
    "* W - матрица весов размера (in_features, out_features)\n",
    "* b - вектор-строка для смещения размера (out_features)\n",
    "* y - выход размера (batch_size, out_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\dfrac{dLinear(x)}{dx} = W^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\dfrac{dLinear(x)}{dW} = x^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\dfrac{dLinear(x)}{db} = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear: # линейный слой без смещения (bias)\n",
    "    def __init__(self, in_features, out_features):\n",
    "        \"\"\"\n",
    "        Конструктор линейного слоя.\n",
    "        \"\"\"\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.prev_input = None # вход с предыдущего слоя\n",
    "\n",
    "        # задаём начальные данные для модели из равномерного распределения от -0.5 до 0.5\n",
    "        self.W = np.random.uniform(low=-0.5, high=0.5, size=(in_features, out_features)) # матрица весов слоя\n",
    "        self.bias = np.random.uniform(low=-0.5, high=0.5, size=(out_features)) # смещение как вектор-строка\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Функция, применяющая веса и смещение к входным данным, чья размерность (batch_size, in_features).\\n\n",
    "        \"\"\"\n",
    "        self.prev_input = x.copy() # запоминаем вход с предыдущего шага\n",
    "\n",
    "        #========== v1 (умножение на веса)\n",
    "        res = np.matmul(x, self.W) # умножаем вход на веса (batch_size, in_features)x(in_features, out_features)=(batch_size, out_features)\n",
    "        #========== v1.1\n",
    "        # res = np.zeros(shape=(x.shape[0], self.out_features))\n",
    "        # for batch in range(x.shape[0]):\n",
    "        #     res[batch] = np.matmul(x[batch], w)\n",
    "        #========== v1.2\n",
    "        # res = np.zeros(shape=(x.shape[0], self.out_features))\n",
    "        # for batch_id, batch in enumerate(x):\n",
    "        #     res[batch_id] = np.matmul(batch, w)\n",
    "        #==========\n",
    "\n",
    "        #========== v1 (добавление смещения)\n",
    "        res = np.add(res, self.bias) # добавляем смещение (bias) для каждого батча\n",
    "        #========== v1.1\n",
    "        # for batch in res:\n",
    "        #     batch += self.bias\n",
    "        #==========\n",
    "        return res\n",
    "        \n",
    "    \n",
    "    def backward(self, grad: np.ndarray, lr: np.float64=0.0001):\n",
    "        \"\"\"\n",
    "        Функция для обратного прохода градиента (обновляющая веса и смещение).\\n\n",
    "        Parameters:\n",
    "            * grad: градиент, пришедший со следующего шага\\n\n",
    "        Returns:\n",
    "            * np.ndarray: градиент с учётом текущего шага, передающийся назад\n",
    "        \"\"\"\n",
    "        # print(\"grad from next step\", grad.shape) # (20, 10)\n",
    "        # print(\"prev_input\", self.prev_input.shape) # (20, 196)\n",
    "        # print(\"prev_input.T\", self.prev_input.T.shape) # (196, 20)\n",
    "        # grad_W = np.matmul(grad, self.prev_input.T) # считаем градиент ошибки по весу W\n",
    "        grad_W = np.matmul(self.prev_input.T, grad) # считаем градиент ошибки по весу W\n",
    "\n",
    "        # print(\"current W\", self.W.shape) # (196, 10)\n",
    "        # print(\"grad_W\", grad_W.shape) # (196, 10)\n",
    "        # self.W = self.W - lr * grad_W.sum(axis=0)/grad_W.shape[0] # обновляем веса (усредняя градиент по батчам)\n",
    "        self.W = self.W - lr * grad_W # обновляем веса\n",
    "\n",
    "        grad_b = grad # считаем градиент ошибки по смещению\n",
    "        self.bias = self.bias - lr * grad_b.sum(axis=0)/grad_b.shape[0] # обновляем смещение (усредняя градиент по батчам)\n",
    "        \n",
    "        grad = np.matmul(grad, self.W.T) # градиент для предыдущего слоя\n",
    "\n",
    "        return grad "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример данных для модели\n",
    "x = torch.randn(size=(10, 5)) # вход\n",
    "w = torch.randn(size=(5, 3)) # вес\n",
    "b = torch.randn(3) # смещение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12423021,  3.14884949, -4.22655594],\n",
       "       [-0.06748438,  0.22652447,  2.73792756],\n",
       "       [-2.12129417, -0.87855899, -1.27546465],\n",
       "       [-0.22169718,  0.44471323,  0.56806123],\n",
       "       [-0.18081808,  1.80725038, -2.91981876],\n",
       "       [-1.18331137, -0.51713824, -3.14313769],\n",
       "       [-0.41436043,  1.81668627, -6.05000532],\n",
       "       [-1.66898718,  2.20227671, -5.34750879],\n",
       "       [ 2.69602641,  0.24410003,  4.17580855],\n",
       "       [-0.42584985,  0.89837806, -1.9942196 ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# вариант по батчам v1\n",
    "res = np.zeros(shape=(10,3))\n",
    "for batch in range(10):\n",
    "    res[batch] = np.matmul(x[batch], w)\n",
    "    res[batch] = np.add(res[batch], b)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12423021,  3.1488495 , -4.226556  ],\n",
       "       [-0.06748438,  0.22652447,  2.7379274 ],\n",
       "       [-2.1212943 , -0.878559  , -1.2754647 ],\n",
       "       [-0.22169718,  0.44471323,  0.56806123],\n",
       "       [-0.18081807,  1.8072503 , -2.9198189 ],\n",
       "       [-1.1833113 , -0.51713824, -3.1431377 ],\n",
       "       [-0.41436043,  1.8166863 , -6.0500054 ],\n",
       "       [-1.6689872 ,  2.2022765 , -5.347509  ],\n",
       "       [ 2.6960263 ,  0.24410003,  4.1758084 ],\n",
       "       [-0.42584985,  0.898378  , -1.9942195 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = Linear(5, 3)\n",
    "linear.W = w.numpy()\n",
    "linear.bias = b.numpy()\n",
    "linear.forward(x.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1242,  3.1488, -4.2266],\n",
       "        [-0.0675,  0.2265,  2.7379],\n",
       "        [-2.1213, -0.8786, -1.2755],\n",
       "        [-0.2217,  0.4447,  0.5681],\n",
       "        [-0.1808,  1.8073, -2.9198],\n",
       "        [-1.1833, -0.5171, -3.1431],\n",
       "        [-0.4144,  1.8167, -6.0500],\n",
       "        [-1.6690,  2.2023, -5.3475],\n",
       "        [ 2.6960,  0.2441,  4.1758],\n",
       "        [-0.4258,  0.8984, -1.9942]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# вариант по батчам v2\n",
    "res = np.matmul(x, w)\n",
    "for batch in res:\n",
    "    batch += b\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1242,  3.1488, -4.2266],\n",
       "        [-0.0675,  0.2265,  2.7379],\n",
       "        [-2.1213, -0.8786, -1.2755],\n",
       "        [-0.2217,  0.4447,  0.5681],\n",
       "        [-0.1808,  1.8073, -2.9198],\n",
       "        [-1.1833, -0.5171, -3.1431],\n",
       "        [-0.4144,  1.8167, -6.0500],\n",
       "        [-1.6690,  2.2023, -5.3475],\n",
       "        [ 2.6960,  0.2441,  4.1758],\n",
       "        [-0.4258,  0.8984, -1.9942]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# однострочный вариант\n",
    "np.add(np.matmul(x, w), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNetwork():\n",
    "    def __init__(self, in_features, out_features):\n",
    "        # 196 = 784/4, то есть уменьшили в 4 раза\n",
    "        self.linear1 = Linear(in_features, int(in_features/4)) # задаём первый линейный слой\n",
    "        self.activation = ReLU() # задаём активацию\n",
    "        self.linear2 = Linear(int(in_features/4), 10) # задаём второй линейный слой\n",
    "        self.softmax = Softmax() # задаём softmax\n",
    "        self.layers = [] # список всех слоёв модели\n",
    "        self.layers.append(self.linear1)\n",
    "        self.layers.append(self.activation)\n",
    "        self.layers.append(self.linear2)\n",
    "        self.layers.append(self.softmax)\n",
    "\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Функция для вызова forward метода всех слоёв модели.\\n\n",
    "        Parameters:\n",
    "            * x: данные на вход размера (batch_size, channels, height, width)\\n\n",
    "        Returns:\n",
    "            * np.ndarray: результат вызова всех слоёв модели\n",
    "        \"\"\"\n",
    "        x = x.reshape(x.shape[0], x.shape[1]*x.shape[2]*x.shape[3]) # \"избавляемся\" от размерностей channels, height, width, совмещаем их в одномерный массив -> получаем двумерный массив размера (batch_size, channels*height*width)\n",
    "        for layer in self.layers: # идём по слоям модели\n",
    "            x = layer.forward(x) # последовательно вызываем forward метод каждого слоя\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def backward(self, grad) -> None:\n",
    "        \"\"\"\n",
    "        Функция для обновления весов модели.\\n\n",
    "        Parameters:\n",
    "            * grad: значение градиента, используемое для обновления весов\\n\n",
    "        Returns:\n",
    "            * None: обновляет веса модели путём градиентного спуска\n",
    "        \"\"\"\n",
    "        for layer in reversed(self.layers): # идём по слоям в обратном порядке\n",
    "            grad = layer.backward(grad) # обновляем веса у слоя и возвращаем изменённый loss \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs: np.int64, train_loader, loss_func, num_classes: np.int64) -> list: # функция обучения\n",
    "    \"\"\"\n",
    "    Функция обучения модели.\\n\n",
    "    Parameters:\n",
    "        * model: модель для обучения\n",
    "        * epochs: число эпох обучения\n",
    "        * train_loader: загрузчик данных\n",
    "        * loss_func: функция для подсчёта потерь\n",
    "        * num_classes: число классов в задаче multi-class classification\\n\n",
    "    Returns:\n",
    "        * list: список с получившимися значениями loss функции\n",
    "    \"\"\"\n",
    "    losses = [0.0] * epochs # заготавливаем массив под значения loss функции\n",
    "    for epoch in range(epochs): # обучаемся по эпохам\n",
    "        for batch_idx, (data, target) in enumerate(train_loader): # идём по батчам, что возвращает train_loader\n",
    "            # переводим target в вероятностное пространство (везде нули, кроме нужного таргета - там единица)\n",
    "            # p_true = np.array([0.0 if i != target else 1.0 for i in range(num_classes)], dtype=np.float64) # переводим target в вероятностное пространство (везде нули, кроме нужного таргета - там единица)\n",
    "            p_true = np.zeros(shape=(train_loader.batch_size, num_classes)) # заготовка под вероятности (пока заполнена нулями)\n",
    "            target  = target.numpy() # переводим таргеты из формата tensor в numpy.array (не обязательно для корректной работы)\n",
    "            for batch, t in enumerate(target): # идём по батчу (нескольким сэмплам), t - id правильного таргета\n",
    "                p_true[batch][t] = 1.0 # ставим вероятность 1 у нужных таргетов (для всех элементов в батче)\n",
    "\n",
    "            data = data.numpy() # переводим данные из формата tensor в numpy.array\n",
    "\n",
    "            p_pred = model.forward(data) # вызываем forward pass модели (предсказываем)\n",
    "\n",
    "            loss = loss_func.calc_loss(p_pred, p_true) # считаем loss модели\n",
    "            losses[epoch] += loss/len(train_loader) # добавляем посчитанный на батче loss (делённый на len(train_loader) - число батчей, для усреднения)\n",
    "\n",
    "            grad = loss_func.backward() # считаем градиент ошибки\n",
    "            model.backward(grad) # обновляем веса модели\n",
    "        print(f\"loss on epoch {epoch+1}:\\t {losses[epoch]}\")\n",
    "    return losses\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 1:\t 2.522919261762606\n",
      "loss on epoch 2:\t 1.00546964854852\n",
      "loss on epoch 3:\t 0.7352474323567225\n",
      "loss on epoch 4:\t 0.5891094329610141\n",
      "loss on epoch 5:\t 0.49557266508353737\n",
      "loss on epoch 6:\t 0.42993253990564423\n",
      "loss on epoch 7:\t 0.38088017569622484\n",
      "loss on epoch 8:\t 0.34268305977707664\n",
      "loss on epoch 9:\t 0.31195462995541856\n",
      "loss on epoch 10:\t 0.28654551123446254\n",
      "loss on epoch 11:\t 0.26519349959695576\n",
      "loss on epoch 12:\t 0.2469942843271259\n",
      "loss on epoch 13:\t 0.2313351906938524\n",
      "loss on epoch 14:\t 0.21772363210833987\n",
      "loss on epoch 15:\t 0.20580565182154886\n",
      "loss on epoch 16:\t 0.19525999800561739\n",
      "loss on epoch 17:\t 0.18586996657304908\n",
      "loss on epoch 18:\t 0.177449030037086\n",
      "loss on epoch 19:\t 0.16982716702066747\n",
      "loss on epoch 20:\t 0.16291206619403076\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10 # число классов (различных цифр)\n",
    "epochs = 20 # число эпох обучения\n",
    "model = CustomNetwork(in_features=np.prod(data_train[0][0].shape), out_features=num_classes) # np.prod(data_train[0][0].shape) - произведение всех размерностей входных данных\n",
    "loss_func = CrossEntropyLoss()\n",
    "losses = train(model=model, epochs=epochs, train_loader=train_loader, loss_func=loss_func, num_classes=num_classes)\n",
    "# loss - кросс-энтропия (перекрёстная энтропия)\n",
    "# CrossEntropyLoss на вход ожидает вероятность класса для всех k классов\n",
    "# то есть массив с вероятностями для каждой из четырёх категорий новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10)) # размер графика\n",
    "plt.title(\"Изменение Cross-Entropy Loss в зависимости от эпохи обучения\") # название графика\n",
    "plt.plot(range(1, epochs+1), losses, label=\"Cross-Entropy Loss\") # построение графика, где range(1, epochs+1) — рассматриваемые значения x, losses — отображаемые значения, label — названия графика\n",
    "plt.xlabel(\"Эпоха\") # подпись по оси x\n",
    "plt.ylabel(\"Значение loss функции\") # подпись по оси y\n",
    "plt.legend() # вывод названий графиков\n",
    "plt.show() # вывод графика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверка работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6450,\n",
       "           1.9305,  1.5996,  1.4978,  0.3395,  0.0340, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  2.4015,\n",
       "           2.8088,  2.8088,  2.8088,  2.8088,  2.6433,  2.0960,  2.0960,\n",
       "           2.0960,  2.0960,  2.0960,  2.0960,  2.0960,  2.0960,  1.7396,\n",
       "           0.2377, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.4286,\n",
       "           1.0268,  0.4922,  1.0268,  1.6505,  2.4651,  2.8088,  2.4396,\n",
       "           2.8088,  2.8088,  2.8088,  2.7578,  2.4906,  2.8088,  2.8088,\n",
       "           1.3577, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.2078,  0.4159, -0.2460,\n",
       "           0.4286,  0.4286,  0.4286,  0.3268, -0.1569,  2.5797,  2.8088,\n",
       "           0.9250, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242,  0.6322,  2.7960,  2.2360,\n",
       "          -0.1951, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.1442,  2.5415,  2.8215,  0.6322,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242,  1.2177,  2.8088,  2.6051,  0.1358,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242,  0.3268,  2.7451,  2.8088,  0.3649, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242,  1.2686,  2.8088,  1.9560, -0.3606, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.3097,  2.1851,  2.7324,  0.3140, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242,  1.1795,  2.8088,  1.8923, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           0.5304,  2.7706,  2.6306,  0.3013, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1824,\n",
       "           2.3887,  2.8088,  1.6887, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860,  2.1596,\n",
       "           2.8088,  2.3633,  0.0213, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0595,  2.8088,\n",
       "           2.8088,  0.5559, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.0296,  2.4269,  2.8088,\n",
       "           1.0395, -0.4115, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242,  1.2686,  2.8088,  2.8088,\n",
       "           0.2377, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242,  0.3522,  2.6560,  2.8088,  2.8088,\n",
       "           0.2377, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242,  1.1159,  2.8088,  2.8088,  2.3633,\n",
       "           0.0849, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242,  1.1159,  2.8088,  2.2105, -0.1951,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test[0][0] # первый sample данных теста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test[0][1] # его таргет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.08632371e-19, 3.57546870e-13, 2.08428080e-10, 3.47743913e-09,\n",
       "        2.09595346e-15, 2.48508789e-14, 1.46832032e-26, 9.99999996e-01,\n",
       "        6.41288705e-11, 1.45392484e-10]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([data_test[0][0]]) # конвертируем данные в формат np.array размера (batch_size, channels, height, width), так как изначально они размера (channels, height, width) и формата tensor\n",
    "pred = model.forward(data) # делаем предсказание вероятностей классов\n",
    "pred # вероятности классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.argmax() # вывод индекса самого вероятного класса (совпадает с таргетом)\n",
    "# pred.argmax(axis=-1) # array([7], dtype=int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На всём тестовом датасете"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Accuracy = \\dfrac{Правильные\\ предсказания}{Все\\ предсказания}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, eval_loader) -> float:\n",
    "    \"\"\"\n",
    "    Функция для подсчёта accuracy.\\n\n",
    "    Parameters:\n",
    "        * model: оцениваемая модель\n",
    "        * eval_loader: загрузчик данных\\n\n",
    "    Returns:\n",
    "        * float: значение accuracy модели\n",
    "    \"\"\"\n",
    "    samples_num = len(eval_loader.dataset) # число объектов в датасете\n",
    "    labels_true = [] # список под настоящие классы\n",
    "    labels_pred = [] # список под предсказаные классы\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(eval_loader): # идём по батчам, что возвращает train_loader\n",
    "        target  = target.numpy() # переводим таргеты из формата tensor в numpy.array\n",
    "        for t in target: # идём по таргетам (их сразу несколько == размер батча в eval_loader)\n",
    "            labels_true.append(t) # добавляем target в список с настоящими классами\n",
    "\n",
    "        data = data.numpy() # переводим данные из формата tensor в numpy.array\n",
    "\n",
    "        preds = model.forward(data) # вызываем forward pass модели (предсказываем)\n",
    "        preds = preds.argmax(axis=-1) # из вероятностей предсказанных классов берём номера самых вероятных (\"предсказанных\")\n",
    "        for pred in preds: # идём по предсказаниям (их сразу несколько == размер батча в eval_loader)\n",
    "            labels_pred.append(pred) # добавляем предсказанный класс в список с предсказаниями\n",
    "    \n",
    "    correct_predictions = 0 # число правильных предсказаний\n",
    "    all_predictions = samples_num # число всех предсказаний, в данном случае совпадает с размером датасета\n",
    "    for i in range(samples_num): # идём по всем sample-ам в датасете\n",
    "        if labels_pred[i] == labels_true[i]: # если предсказанный класс совпадает с правильным\n",
    "            correct_predictions += 1 # увеличиваем число совпадений\n",
    "    accuracy = correct_predictions/all_predictions # считаем accuracy\n",
    "    return accuracy # возвращаем accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9404"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(model=model, eval_loader=test_loader) # подсчёт accuracy на всём датасете"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
