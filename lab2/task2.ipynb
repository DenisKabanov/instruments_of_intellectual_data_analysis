{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решить задачу классификации рукописных цифр на датасете mnist https://www.kaggle.com/datasets/hojjatk/mnist-dataset. Правила следующие:\n",
    "- нужно представить решение в виде нейронной сети, написанной на numpy, и обученной с помощью алгоритма градиентного спуска;\n",
    "- нейронная сеть должна состоять из двух линейных слоев, активаций relu и softmax, и mse лосса;\n",
    "- нельзя пользоваться автоградиентом (pytorch, numpy). Градиенты должны считаться вручную по алгоритму обратного распространения ошибки, используя аналитические формулы производных;\n",
    "- решение считается валидным, если оно достигает аккураси больше 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройки/Гиперпараметры/Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # для вывода графиков/картинок\n",
    "import numpy as np # для работы с матрицами\n",
    "\n",
    "import torchvision # для работы с картинками (преобразований)\n",
    "import torch # для создания модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка и обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# преобразования над датасетом\n",
    "transforms = torchvision.transforms.Compose([ # Compose объединяет несколько преобразований вместе, чтобы они выполнялись \"последовательно\"\n",
    "    torchvision.transforms.ToTensor(), # преобразование PIL изображения (или ndarray формата (Height x Width x Channels)) в tensor (типа float со значениями в области [0.0, 1.0], если такая трансформация поддерживается (см описание ToTensor))\n",
    "    torchvision.transforms.Normalize(mean=(0.1307,), std=(0.3081,)) # нормализация каналов (в датасете всего один канал) к указанным среднему значению и стандартному отклонению (цифры подобраны под датасет)\n",
    "])\n",
    "\n",
    "# датасет\n",
    "data_train = torchvision.datasets.MNIST(root=\"./data\", # путь, откуда брать/куда сохранять датасет\n",
    "                                        train=True, # скачиание обучающей части датасета\n",
    "                                        download=True, # скачивать ли датасет, если его нет в root пути\n",
    "                                        transform=transforms # функция, принимающая на вход PIL Image и преобразовывающая его\n",
    "                                       )\n",
    "\n",
    "# загрузчик данных для обучения\n",
    "train_loader = torch.utils.data.DataLoader(dataset=data_train, # указание датасета для DataLoader\n",
    "                                           batch_size=20, # размер батчка (число сэмплов, что будет возвращать DataLoader за раз) (градиент усредняется по батчу, ускоряется обработка датасета, но слегка замедляется обработка сэмпла)\n",
    "                                           num_workers=5, # число используемых ядер процессора для ускорения обработки данных\n",
    "                                           pin_memory=True # нужно ли заранее аллоцировать память под объект на GPU (лучше так, чем возвращать CUDA tensors при multi-process loading)\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train # данные о датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860, -0.1951,\n",
       "          -0.1951, -0.1951,  1.1795,  1.3068,  1.8032, -0.0933,  1.6887,\n",
       "           2.8215,  2.7197,  1.1923, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.0424,  0.0340,  0.7722,  1.5359,  1.7396,  2.7960,\n",
       "           2.7960,  2.7960,  2.7960,  2.7960,  2.4396,  1.7650,  2.7960,\n",
       "           2.6560,  2.0578,  0.3904, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           0.1995,  2.6051,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
       "           2.7960,  2.7960,  2.7960,  2.7706,  0.7595,  0.6195,  0.6195,\n",
       "           0.2886,  0.0722, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.1951,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
       "           2.0960,  1.8923,  2.7197,  2.6433, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242,  0.5940,  1.5614,  0.9377,  2.7960,  2.7960,  2.1851,\n",
       "          -0.2842, -0.4242,  0.1231,  1.5359, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.2460, -0.4115,  1.5359,  2.7960,  0.7213,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242,  1.3450,  2.7960,  1.9942,\n",
       "          -0.3988, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.2842,  1.9942,  2.7960,\n",
       "           0.4668, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0213,  2.6433,\n",
       "           2.4396,  1.6123,  0.9504, -0.4115, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6068,\n",
       "           2.6306,  2.7960,  2.7960,  1.0904, -0.1060, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           0.1486,  1.9432,  2.7960,  2.7960,  1.4850, -0.0806, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.2206,  0.7595,  2.7833,  2.7960,  1.9560, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242,  2.7451,  2.7960,  2.7451,  0.3904,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           0.1613,  1.2305,  1.9051,  2.7960,  2.7960,  2.2105, -0.3988,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0722,  1.4596,\n",
       "           2.4906,  2.7960,  2.7960,  2.7960,  2.7578,  1.8923, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.1187,  1.0268,  2.3887,  2.7960,\n",
       "           2.7960,  2.7960,  2.7960,  2.1342,  0.5686, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.1315,  0.4159,  2.2869,  2.7960,  2.7960,  2.7960,\n",
       "           2.7960,  2.0960,  0.6068, -0.3988, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1951,\n",
       "           1.7523,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.0578,\n",
       "           0.5940, -0.3097, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242,  0.2758,  1.7650,  2.4524,\n",
       "           2.7960,  2.7960,  2.7960,  2.7960,  2.6815,  1.2686, -0.2842,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242,  1.3068,  2.7960,  2.7960,\n",
       "           2.7960,  2.2742,  1.2941,  1.2559, -0.2206, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0][0] # первое \"изображение\" датасета после применения трансформаций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0][1] # класс (target) первого изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data_train[0][0].reshape((28, 28, 1)), cmap='gray') # построение изображения\n",
    "# .reshape((28, 28, 1)), так как shape(1, 28, 28) ~ (Channels, Height, Width) не воспринимается\n",
    "# cmap='gray' так как изображение одноканальное, то есть чёрно-белое\n",
    "plt.show() # вывод изображения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Cross-Entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Cross Entropy Loss(p^{pred}) = \\dfrac{1}{batch\\_size} \\sum_{N=1}^{batch\\_size} \\sum_{C=1}^{classes} -p^{true}_{N,C}*log_e(p^{pred}_{N,C})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\dfrac{dCross Entropy Loss(p^{pred})}{dp^{pred}_{C}} = \\dfrac{1}{batch\\_size} \\sum_{N=1}^{batch\\_size} -\\dfrac{p^{true}_{N,C}}{p^{pred}_{N,C}} + \\dfrac{1-p^{true}_{N,C}}{1-p^{pred}_{N,C}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Без усреднения по батчу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Cross Entropy Loss(p^{pred}_N) = \\sum_{C=1}^{classes} -p^{true}_{N,C}*log_e(p^{pred}_{N,C}),\\ где\\ N\\ -\\ номер\\ батча\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\dfrac{dCross Entropy Loss(p^{pred}_N)}{dp^{pred}_{N,C}} = -\\dfrac{p^{true}_{N,C}}{p^{pred}_{N,C}} + \\dfrac{1-p^{true}_{N,C}}{1-p^{pred}_{N,C}},\\ где\\ N\\ -\\ номер\\ батча\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Конструктор функции для подсчёта CrossEntropyLoss.\n",
    "        \"\"\"\n",
    "        self.loss = None # значение подсчитанного loss\n",
    "        self.grad = None # градиент размера (batch_size, classes)\n",
    "        self.p_pred = None # \n",
    "        self.p_true = None\n",
    "        self.batch_size = 1 # размер батча\n",
    "        self.classes = 1 # число классов\n",
    "\n",
    "    def calc_loss(self, p_pred, p_true) -> np.float64: # результат разниться с torch.nn.CrossEntropyLoss()!\n",
    "        \"\"\"\n",
    "        Функция для подсчёта Cross-Entropy loss с усреднением по батчу.\\n\n",
    "        Parameters:\n",
    "            * p_pred: предсказанные вероятности классов размера (batch_size, classes)\n",
    "            * p_true: реальные вероятности классов размера (batch_size, classes)\\n\n",
    "        Returns:\n",
    "            * np.float64: значение функции потерь\n",
    "        \"\"\"\n",
    "        self.batch_size = p_true.shape[0] # размер батча\n",
    "        self.classes = p_true.shape[1] # число классов\n",
    "        loss = 0.0 # значение loss\n",
    "\n",
    "        # workaround для того, чтобы избавиться от inf и nan\n",
    "        p_pred[p_pred==0.0] = 0.000001 # заменяем полностью нулевые вероятности на очень малые - чтобы логарифм в формуле не давал -inf\n",
    "        p_pred[p_pred==1.0] = 0.999999 # заменяем вероятности в 1 на очень высокие - чтобы в backward в формуле (1-self.p_true[batch][c])/(1-self.p_pred[batch][c])  не получился NaN\n",
    "        p_true[p_true==0.0] = 0.000001 # заменяем полностью нулевые вероятности на очень малые - чтобы логарифм в формуле не давал -inf\n",
    "        p_true[p_true==1.0] = 0.999999 # заменяем вероятности в 1 на очень высокие - чтобы в backward в формуле (1-self.p_true[batch][c])/(1-self.p_pred[batch][c])  не получился NaN\n",
    "\n",
    "        for batch in range(self.batch_size): # идём по числу батчей (внешний цикл)\n",
    "            #========== v1\n",
    "            loss += np.matmul(p_true[batch], np.log(p_pred[batch])) # сумма по классам на определённом батче\n",
    "            #========== v2\n",
    "            # for c in range(classes):\n",
    "            #     loss += p_true[batch][c] * np.log(p_pred[batch][c])\n",
    "            #==========\n",
    "        loss = - loss / self.batch_size # домножаем на -1 и берём среднее по батчам\n",
    "\n",
    "        self.loss = loss # запоминаем подсчитанный loss\n",
    "\n",
    "        #========== v1 (без усреднения градиента по батчам)\n",
    "        self.grad = np.zeros(shape=(self.batch_size, self.classes)) # заготовка под матрицу градиентов\n",
    "        #========== v2 (с усреднением градиента по батчам)\n",
    "        # self.grad = np.zeros(shape=(self.classes)) # заготовка под матрицу градиентов (с усреднением по батчам)\n",
    "        #==========\n",
    "\n",
    "        self.p_pred = p_pred\n",
    "        self.p_true = p_true\n",
    "        return loss # возвращаем посчитанный loss\n",
    "    \n",
    "\n",
    "    def backward(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Функция для подсчёта градиента после Cross-Entropy loss.\\n\n",
    "        Returns:\n",
    "            * np.ndarray: значение градиента размер (batch_size, classes)\n",
    "        \"\"\"\n",
    "        #========== v1 (если считать, что вход пришёл от softmax, то сразу возвращаем dloss/dsoftmax_input)\n",
    "        # for batch in range(self.batch_size): # идём по номерам батчей\n",
    "        #     #========== v1 с итерированием по классам\n",
    "        #     for c in range(self.classes): # идём по классам\n",
    "        #         self.grad[batch][c] = self.p_pred[batch][c] - self.p_true[batch][c] # считаем градиент при условии, что вход от softmax (см grad.png)\n",
    "        #     #========== v2 без итерирования по классам (работаем с векторами-строками)\n",
    "        #     self.grad[batch] = self.p_pred[batch] - self.p_true[batch] # считаем градиент при условии, что вход от softmax\n",
    "        #     #==========\n",
    "        #========== v1.1 (v1 с усреднением по батчам)\n",
    "        # for batch in range(self.batch_size): # идём по номерам батчей\n",
    "        #     #========== v1 с итерированием по классам\n",
    "        #     # for c in range(self.classes): # идём по классам\n",
    "        #     #     self.grad[c] += self.p_pred[batch][c] - self.p_true[batch][c] # считаем градиент (см grad.png)\n",
    "        #     #========== v2 без итерирования по классам (работаем с векторами-строками)\n",
    "        #     self.grad += self.p_pred[batch] - self.p_true[batch] # считаем градиент\n",
    "        #     #==========\n",
    "        # self.grad = self.grad / self.batch_size # усредняем градиент по числу батчей\n",
    "        #========== v2 (общий случай)\n",
    "        for batch in range(self.batch_size): # идём по номерам батчей\n",
    "            #========== v1 с итерированием по классам\n",
    "            # for c in range(self.classes): # идём по классам\n",
    "            #     self.grad[batch][c] = -self.p_true[batch][c]/self.p_pred[batch][c] + (1-self.p_true[batch][c])/(1-self.p_pred[batch][c])\n",
    "            #========== v2 без итерирования по классам (работаем с векторами-строками)\n",
    "            self.grad[batch] = -(self.p_true[batch]/self.p_pred[batch]) + (1-self.p_true[batch])/(1-self.p_pred[batch]) # считаем градиент от предсказанных вероятностей\n",
    "            #==========\n",
    "        #========== v2.1 (v2 с усреднением по батчам)\n",
    "        # for batch in range(self.batch_size): # идём по номерам батчей\n",
    "        #     #========== v1 с итерированием по классам\n",
    "        #     # for c in range(self.classes): # идём по классам\n",
    "        #     #     self.grad[c] += -(self.p_true[batch][c]/self.p_pred[batch][c]) + (1-self.p_true[batch][c])/(1-self.p_pred[batch][c]) # считаем градиент от предсказанных вероятностей\n",
    "        #     #========== v2 без итерирования по классам (работаем с векторами-строками)\n",
    "        #     self.grad += -(self.p_true[batch]/self.p_pred[batch]) + (1-self.p_true[batch])/(1-self.p_pred[batch]) # считаем градиент от предсказанных вероятностей\n",
    "        #     #==========\n",
    "        # self.grad = self.grad / self.batch_size # усредняем градиент по числу батчей\n",
    "        #==========\n",
    "\n",
    "        return self.grad # возвращаем посчитанный градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9009, dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_pred = np.array([[0.2,0.6,0.2], [1.0,0.0,0.0]])\n",
    "# probs_pred = np.array([[1.0,0.0,0.0], [1.0,0.0,0.0]])\n",
    "probs_pred = torch.tensor(probs_pred)\n",
    "\n",
    "probs_true = np.array([[1.0,0.0,0.0], [1.0,0.0,0.0]])\n",
    "probs_true = torch.tensor(probs_true)\n",
    "\n",
    "loss_torch = torch.nn.CrossEntropyLoss()\n",
    "loss_torch(probs_pred, probs_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8047, dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_custom = CrossEntropyLoss()\n",
    "loss_custom.calc_loss(probs_pred, probs_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.99999375,  2.49999583,  1.24999375],\n",
       "       [ 0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_custom.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "ReLU(x) =  \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      0 & if & x \\leq 0 \\\\\n",
    "      x & if & x > 0 \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\dfrac{dReLU(x)}{dx} =  \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      0 & if & x \\leq 0 \\\\\n",
    "      1 & if & x > 0 \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"pics/ReLU.png\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Конструктор ReLU функции.\n",
    "        \"\"\"\n",
    "        self.prev_input = None # вход с предыдущего слоя\n",
    "\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray: # numpy.array — это просто удобная функция для создания numpy.ndarray\n",
    "        \"\"\"\n",
    "        Функция активации ReLU, если значение в X меньше или равно нулю - оно становится нулём, иначе — остаётся прежним.\\n\n",
    "        Parameters:\n",
    "            * x: данные в виде массива размера (batch_size, features)\\n\n",
    "        Returns:\n",
    "            * np.ndarray: преобразованный x согласно работе функции активации\n",
    "        \"\"\"\n",
    "        self.prev_input = x.copy() # запоминаем вход с предыдущего шага\n",
    "        x[x<0] = 0.0 # заменяем все значения в x, что меньше нуля на ноль\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Функция для обратного прохода градиента.\\n\n",
    "        Parameters:\n",
    "            * grad: градиент, пришедший со следующего шага\\n\n",
    "        Returns:\n",
    "            * np.ndarray: градиент с учётом текущего шага, передающийся назад\n",
    "        \"\"\"\n",
    "        grad[self.prev_input < 0] = 0 # зануляем градиент там, где вход был меньше нуля\n",
    "        # как бы поэлементно умножаем пришедший градиент на матрицу из нулей и единиц (так как это производная ReLU) \n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Softmax(x) = \\dfrac{e^{x_i}}{\\sum_{j}e^{x_j}} = p_i,\\ где\\ i=\\overline{1, classes}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\dfrac{dSoftmax(x)}{dx_i} = \\dfrac{e^{x_i}}{\\sum_{j}e^{x_j}} (1 - \\dfrac{e^{x_i}}{\\sum_{j}e^{x_j}}) = p_i * (1 - p_i),\\ где\\ i=\\overline{1, classes}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Конструктор Softmax функции.\n",
    "        \"\"\"\n",
    "        # print(\"constructed\")\n",
    "        self.prev_input = None # вход с предыдущего слоя\n",
    "        self.prev_out = None # выход со слоя\n",
    "\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Функция Softmax, пересчитывающая для всех элементов массива x значение e^x_i/summ(e^x_i).\\n\n",
    "        Parameters:\n",
    "            * x: данные в виде массива размерности (batch_size, class_count) ~ (размер батча, число классов)\\n\n",
    "        Returns:\n",
    "            * np.ndarray: преобразованный X согласно формуле Softmax\n",
    "        \"\"\"\n",
    "        prob = np.exp(x) # считаем экспоненту от всех элементов X (каждый элемент в X идёт как степень e)\n",
    "\n",
    "        for batch in range(prob.shape[0]): # идём по батчам\n",
    "            prob[batch] = prob[batch] / prob[batch].sum() # каждый элемент (экспоненту) в батче делим на сумму экспонент этого батча\n",
    "        self.prev_out = prob.copy() # запоминаем выход слоя\n",
    "        # print(\"prob\\n\", prob) # DEBUG\n",
    "        return prob # возвращаем результат Softmax\n",
    "    \n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray: # grad = dloss/dinput = dloss/dprob * dprob/dinput, где prob - выход слоя softmax\n",
    "        \"\"\"\n",
    "        Функция для обратного прохода градиента.\\n\n",
    "        Parameters:\n",
    "            * grad: градиент, пришедший со следующего шага\\n\n",
    "        Returns:\n",
    "            * np.ndarray: градиент с учётом текущего шага, передающийся назад\n",
    "        \"\"\"\n",
    "        #========== v1 (если в CrossEntropyLoss учитывается то, что вход от softmax ==> грубо говоря - пропускаем шаг с подсчётом dloss/dprob, а сразу считаем dloss/dinput)\n",
    "        # return grad # просто возвращаем полученный градиент (при условии формулы в CrossEntropyLoss)\n",
    "        #========== v2 (общий случай)\n",
    "        return grad * self.prev_out * (1 - self.prev_out) # домножаем полученный градиент на значение производной (dot product - поэлементное умножение)\n",
    "        #=========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.99833498e-01 6.00333004e-01 1.99833498e-01]\n",
      " [9.99909208e-01 4.53958078e-05 4.53958078e-05]]\n",
      "[[-8.00165502e-01  6.00332004e-01  1.99832498e-01]\n",
      " [-8.97916157e-05  4.43958078e-05  4.43958078e-05]]\n"
     ]
    }
   ],
   "source": [
    "# probs_pred = np.array([[0.2,0.6,0.2], [1.0,0.0,0.0]])\n",
    "# probs_pred = np.array([[1.0,0.0,0.0], [1.0,0.0,0.0]])\n",
    "input = np.array([[2.0,3.1,2.0], [10.0,0.0,0.0]]) # ==> probs_pred ~ [[0.2,0.6,0.2], [1.0,0.0,0.0]]\n",
    "\n",
    "probs_true = np.array([[1.0,0.0,0.0], [1.0,0.0,0.0]])\n",
    "\n",
    "soft = Softmax()\n",
    "res = soft.forward(input)\n",
    "print(res)\n",
    "\n",
    "loss_custom = CrossEntropyLoss()\n",
    "loss_custom.calc_loss(res, np.array([[1.0,0.0,0.0], [1.0,0.0,0.0]]))\n",
    "grad = loss_custom.backward()\n",
    "print(soft.backward(grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Linear(x) = xW + b\n",
    "$$\n",
    "* x - вход размера (batch_size, in_features)\n",
    "* W - матрица весов размера (in_features, out_features)\n",
    "* b - вектор-строка для смещения размера (out_features)\n",
    "* y - выход размера (batch_size, out_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\dfrac{dLinear(x)}{dx} = W^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\dfrac{dLinear(x)}{dW} = x^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\dfrac{dLinear(x)}{db} = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear: # линейный слой без смещения (bias)\n",
    "    def __init__(self, in_features, out_features):\n",
    "        \"\"\"\n",
    "        Конструктор линейного слоя.\n",
    "        \"\"\"\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.prev_input = None # вход с предыдущего слоя\n",
    "\n",
    "        # задаём начальные данные для модели из равномерного распределения от -0.5 до 0.5\n",
    "        self.W = np.random.uniform(low=-0.5, high=0.5, size=(in_features, out_features)) # матрица весов слоя\n",
    "        self.bias = np.random.uniform(low=-0.5, high=0.5, size=(out_features)) # смещение как вектор-строка\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Функция, применяющая веса и смещение к входным данным, чья размерность (batch_size, in_features).\\n\n",
    "        \"\"\"\n",
    "        self.prev_input = x.copy() # запоминаем вход с предыдущего шага\n",
    "\n",
    "        #========== v1 (умножение на веса)\n",
    "        res = np.matmul(x, self.W) # умножаем вход на веса (batch_size, in_features)x(in_features, out_features)=(batch_size, out_features)\n",
    "        #========== v1.1\n",
    "        # res = np.zeros(shape=(x.shape[0], self.out_features))\n",
    "        # for batch in range(x.shape[0]):\n",
    "        #     res[batch] = np.matmul(x[batch], w)\n",
    "        #========== v1.2\n",
    "        # res = np.zeros(shape=(x.shape[0], self.out_features))\n",
    "        # for batch_id, batch in enumerate(x):\n",
    "        #     res[batch_id] = np.matmul(batch, w)\n",
    "        #==========\n",
    "\n",
    "        #========== v1 (добавление смещения)\n",
    "        res = np.add(res, self.bias) # добавляем смещение (bias) для каждого батча\n",
    "        #========== v1.1\n",
    "        # for batch in res:\n",
    "        #     batch += self.bias\n",
    "        #==========\n",
    "        return res\n",
    "        \n",
    "    \n",
    "    def backward(self, grad: np.ndarray, lr: np.float64=0.0001):\n",
    "        \"\"\"\n",
    "        Функция для обратного прохода градиента (обновляющая веса и смещение).\\n\n",
    "        Parameters:\n",
    "            * grad: градиент, пришедший со следующего шага\\n\n",
    "        Returns:\n",
    "            * np.ndarray: градиент с учётом текущего шага, передающийся назад\n",
    "        \"\"\"\n",
    "        # print(\"grad from next step\", grad.shape) # (20, 10)\n",
    "        # print(\"prev_input\", self.prev_input.shape) # (20, 196)\n",
    "        # print(\"prev_input.T\", self.prev_input.T.shape) # (196, 20)\n",
    "        # grad_W = np.matmul(grad, self.prev_input.T) # считаем градиент ошибки по весу W\n",
    "        grad_W = np.matmul(self.prev_input.T, grad) # считаем градиент ошибки по весу W\n",
    "\n",
    "        # print(\"current W\", self.W.shape) # (196, 10)\n",
    "        # print(\"grad_W\", grad_W.shape) # (196, 10)\n",
    "        # self.W = self.W - lr * grad_W.sum(axis=0)/grad_W.shape[0] # обновляем веса (усредняя градиент по батчам)\n",
    "        self.W = self.W - lr * grad_W # обновляем веса\n",
    "\n",
    "        grad_b = grad # считаем градиент ошибки по смещению\n",
    "        self.bias = self.bias - lr * grad_b.sum(axis=0)/grad_b.shape[0] # обновляем смещение (усредняя градиент по батчам)\n",
    "        # self.bias = self.bias - lr * grad_b\n",
    "        \n",
    "        grad = np.matmul(grad, self.W.T) # градиент для предыдущего слоя\n",
    "\n",
    "        return grad "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример данных для модели\n",
    "x = torch.randn(size=(10, 5)) # вход\n",
    "w = torch.randn(size=(5, 3)) # вес\n",
    "b = torch.randn(3) # смещение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.40002194,  3.72322685,  0.40824163],\n",
       "       [-0.79332903,  2.9594155 , -1.09498012],\n",
       "       [ 0.65042424,  0.94367045,  0.35207057],\n",
       "       [ 3.19763038,  1.75812763, -0.09431708],\n",
       "       [-1.57512549, -0.4743821 ,  4.38424957],\n",
       "       [-1.87161806,  3.35738999, -1.30589068],\n",
       "       [-2.19743207, -1.08883315, -0.24993384],\n",
       "       [ 2.70750448, -4.24223179,  2.55342877],\n",
       "       [ 0.86797532,  1.01374426, -0.6921711 ],\n",
       "       [-0.78623191,  0.98088628,  0.16687959]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# вариант по батчам v1\n",
    "res = np.zeros(shape=(10,3))\n",
    "for batch in range(10):\n",
    "    res[batch] = np.matmul(x[batch], w)\n",
    "    res[batch] = np.add(res[batch], b)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.400022  ,  3.7232268 ,  0.40824163],\n",
       "       [-0.7933289 ,  2.9594154 , -1.0949801 ],\n",
       "       [ 0.65042424,  0.94367045,  0.35207057],\n",
       "       [ 3.1976304 ,  1.7581277 , -0.09431708],\n",
       "       [-1.5751255 , -0.4743821 ,  4.3842497 ],\n",
       "       [-1.871618  ,  3.35739   , -1.3058907 ],\n",
       "       [-2.197432  , -1.0888331 , -0.24993384],\n",
       "       [ 2.7075043 , -4.242232  ,  2.5534286 ],\n",
       "       [ 0.86797535,  1.0137442 , -0.69217134],\n",
       "       [-0.7862319 ,  0.9808863 ,  0.1668796 ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = Linear(5, 3)\n",
    "linear.W = w.numpy()\n",
    "linear.bias = b.numpy()\n",
    "linear.forward(x.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4000,  3.7232,  0.4082],\n",
       "        [-0.7933,  2.9594, -1.0950],\n",
       "        [ 0.6504,  0.9437,  0.3521],\n",
       "        [ 3.1976,  1.7581, -0.0943],\n",
       "        [-1.5751, -0.4744,  4.3842],\n",
       "        [-1.8716,  3.3574, -1.3059],\n",
       "        [-2.1974, -1.0888, -0.2499],\n",
       "        [ 2.7075, -4.2422,  2.5534],\n",
       "        [ 0.8680,  1.0137, -0.6922],\n",
       "        [-0.7862,  0.9809,  0.1669]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# вариант по батчам v2\n",
    "res = np.matmul(x, w)\n",
    "for batch in res:\n",
    "    batch += b\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4000,  3.7232,  0.4082],\n",
       "        [-0.7933,  2.9594, -1.0950],\n",
       "        [ 0.6504,  0.9437,  0.3521],\n",
       "        [ 3.1976,  1.7581, -0.0943],\n",
       "        [-1.5751, -0.4744,  4.3842],\n",
       "        [-1.8716,  3.3574, -1.3059],\n",
       "        [-2.1974, -1.0888, -0.2499],\n",
       "        [ 2.7075, -4.2422,  2.5534],\n",
       "        [ 0.8680,  1.0137, -0.6922],\n",
       "        [-0.7862,  0.9809,  0.1669]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# однострочный вариант\n",
    "np.add(np.matmul(x, w), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNetwork():\n",
    "    def __init__(self, in_features, out_features):\n",
    "        # 196 = 784/4, то есть уменьшили в 4 раза\n",
    "        self.linear1 = Linear(in_features, int(in_features/4)) # задаём первый линейный слой\n",
    "        self.activation = ReLU() # задаём активацию\n",
    "        self.linear2 = Linear(int(in_features/4), 10) # задаём второй линейный слой\n",
    "        self.softmax = Softmax() # задаём softmax\n",
    "        self.layers = [] # список всех слоёв модели\n",
    "        self.layers.append(self.linear1)\n",
    "        self.layers.append(self.activation)\n",
    "        self.layers.append(self.linear2)\n",
    "        self.layers.append(self.softmax)\n",
    "\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Функция для вызова forward метода всех слоёв модели.\\n\n",
    "        Parameters:\n",
    "            * x: данные на вход размера (batch_size, channels, height, width)\\n\n",
    "        Returns:\n",
    "            * np.ndarray: результат вызова всех слоёв модели\n",
    "        \"\"\"\n",
    "        x = x.reshape(x.shape[0], x.shape[1]*x.shape[2]*x.shape[3]) # \"избавляемся\" от размерностей channels, height, width, совмещаем их в одномерный массив -> получаем двумерный массив размера (batch_size, channels*height*width)\n",
    "        for layer in self.layers: # идём по слоям модели\n",
    "            x = layer.forward(x) # последовательно вызываем forward метод каждого слоя\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def backward(self, grad) -> None:\n",
    "        \"\"\"\n",
    "        Функция для обновления весов модели.\\n\n",
    "        Parameters:\n",
    "            * grad: значение градиента, используемое для обновления весов\\n\n",
    "        Returns:\n",
    "            * None: обновляет веса модели путём градиентного спуска\n",
    "        \"\"\"\n",
    "        for layer in reversed(self.layers): # идём по слоям в обратном порядке\n",
    "            grad = layer.backward(grad) # обновляем веса у слоя и возвращаем изменённый loss \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs: np.int64, train_loader, loss_func, num_classes: np.int64) -> list: # функция обучения\n",
    "    \"\"\"\n",
    "    Функция обучения модели.\\n\n",
    "    Parameters:\n",
    "        * model: модель для обучения\n",
    "        * epochs: число эпох обучения\n",
    "        * train_loader: загрузчик данных\n",
    "        * loss_func: функция для подсчёта потерь\n",
    "        * num_classes: число классов в задаче multi-class classification\\n\n",
    "    Returns:\n",
    "        * list: список с получившимися значениями loss функции\n",
    "    \"\"\"\n",
    "    losses = [0.0] * epochs # заготавливаем массив под значения loss функции\n",
    "    for epoch in range(epochs): # обучаемся по эпохам\n",
    "        for batch_idx, (data, target) in enumerate(train_loader): # идём по батчам, что возвращает train_loader\n",
    "            # переводим target в вероятностное пространство (везде нули, кроме нужного таргета - там единица)\n",
    "            # p_true = np.array([0.0 if i != target else 1.0 for i in range(num_classes)], dtype=np.float64) # переводим target в вероятностное пространство (везде нули, кроме нужного таргета - там единица)\n",
    "            p_true = np.zeros(shape=(train_loader.batch_size, num_classes)) # заготовка под вероятности (пока заполнена нулями)\n",
    "            target  = target.numpy() # переводим таргеты из формата tensor в numpy.array (не обязательно для корректной работы)\n",
    "            for batch, t in enumerate(target): # идём по батчу (нескольким сэмплам), t - id правильного таргета\n",
    "                p_true[batch][t] = 1.0 # ставим вероятность 1 у нужных таргетов (для всех элементов в батче)\n",
    "\n",
    "            data = data.numpy() # переводим данные из формата tensor в numpy.array\n",
    "\n",
    "            p_pred = model.forward(data) # вызываем forward pass модели (предсказываем)\n",
    "\n",
    "            loss = loss_func.calc_loss(p_pred, p_true) # считаем loss модели\n",
    "            losses[epoch] += loss/len(train_loader) # добавляем посчитанный на батче loss (делённый на len(train_loader) - число батчей, для усреднения)\n",
    "\n",
    "            grad = loss_func.backward() # считаем градиент ошибки\n",
    "            model.backward(grad) # обновляем веса модели\n",
    "        print(f\"loss on epoch {epoch+1}:\\t {losses[epoch]}\")\n",
    "    return losses\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 1:\t 2.7330246818042876\n",
      "loss on epoch 2:\t 1.065590722833856\n",
      "loss on epoch 3:\t 0.7811828120792682\n",
      "loss on epoch 4:\t 0.6251500013702933\n",
      "loss on epoch 5:\t 0.5238902028779474\n",
      "loss on epoch 6:\t 0.4515529731177466\n",
      "loss on epoch 7:\t 0.39764917694358837\n",
      "loss on epoch 8:\t 0.35588116489825705\n",
      "loss on epoch 9:\t 0.32247302389660953\n",
      "loss on epoch 10:\t 0.29521448355342084\n",
      "loss on epoch 11:\t 0.27256255906966625\n",
      "loss on epoch 12:\t 0.25348456905169303\n",
      "loss on epoch 13:\t 0.23715077518790362\n",
      "loss on epoch 14:\t 0.22303128191781682\n",
      "loss on epoch 15:\t 0.21070041683765936\n",
      "loss on epoch 16:\t 0.1997755562841075\n",
      "loss on epoch 17:\t 0.1900182632771243\n",
      "loss on epoch 18:\t 0.1812739544086199\n",
      "loss on epoch 19:\t 0.1733907329009369\n",
      "loss on epoch 20:\t 0.16625082577885822\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10 # число классов (различных цифр)\n",
    "epochs = 20 # число эпох обучения\n",
    "model = CustomNetwork(in_features=np.prod(data_train[0][0].shape), out_features=num_classes) # np.prod(data_train[0][0].shape) - произведение всех размерностей входных данных\n",
    "loss_func = CrossEntropyLoss()\n",
    "losses = train(model=model, epochs=epochs, train_loader=train_loader, loss_func=loss_func, num_classes=num_classes)\n",
    "# loss - кросс-энтропия (перекрёстная энтропия)\n",
    "# CrossEntropyLoss на вход ожидает вероятность класса для всех k классов\n",
    "# то есть массив с вероятностями для каждой из четырёх категорий новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10)) # размер графика\n",
    "plt.title(\"Изменение Cross-Entropy Loss в зависимости от эпохи обучения\") # название графика\n",
    "plt.plot(range(1, epochs+1), losses, label=\"Cross-Entropy Loss\") # построение графика, где range(1, epochs+1) — рассматриваемые значения x, losses — отображаемые значения, label — названия графика\n",
    "plt.xlabel(\"Эпоха\") # подпись по оси x\n",
    "plt.ylabel(\"Значение loss функции\") # подпись по оси y\n",
    "plt.legend() # вывод названий графиков\n",
    "plt.show() # вывод графика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверка работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860, -0.1951,\n",
       "          -0.1951, -0.1951,  1.1795,  1.3068,  1.8032, -0.0933,  1.6887,\n",
       "           2.8215,  2.7197,  1.1923, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.0424,  0.0340,  0.7722,  1.5359,  1.7396,  2.7960,\n",
       "           2.7960,  2.7960,  2.7960,  2.7960,  2.4396,  1.7650,  2.7960,\n",
       "           2.6560,  2.0578,  0.3904, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           0.1995,  2.6051,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
       "           2.7960,  2.7960,  2.7960,  2.7706,  0.7595,  0.6195,  0.6195,\n",
       "           0.2886,  0.0722, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.1951,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
       "           2.0960,  1.8923,  2.7197,  2.6433, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242,  0.5940,  1.5614,  0.9377,  2.7960,  2.7960,  2.1851,\n",
       "          -0.2842, -0.4242,  0.1231,  1.5359, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.2460, -0.4115,  1.5359,  2.7960,  0.7213,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242,  1.3450,  2.7960,  1.9942,\n",
       "          -0.3988, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.2842,  1.9942,  2.7960,\n",
       "           0.4668, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0213,  2.6433,\n",
       "           2.4396,  1.6123,  0.9504, -0.4115, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6068,\n",
       "           2.6306,  2.7960,  2.7960,  1.0904, -0.1060, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           0.1486,  1.9432,  2.7960,  2.7960,  1.4850, -0.0806, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.2206,  0.7595,  2.7833,  2.7960,  1.9560, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242,  2.7451,  2.7960,  2.7451,  0.3904,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           0.1613,  1.2305,  1.9051,  2.7960,  2.7960,  2.2105, -0.3988,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0722,  1.4596,\n",
       "           2.4906,  2.7960,  2.7960,  2.7960,  2.7578,  1.8923, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.1187,  1.0268,  2.3887,  2.7960,\n",
       "           2.7960,  2.7960,  2.7960,  2.1342,  0.5686, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.1315,  0.4159,  2.2869,  2.7960,  2.7960,  2.7960,\n",
       "           2.7960,  2.0960,  0.6068, -0.3988, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1951,\n",
       "           1.7523,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.0578,\n",
       "           0.5940, -0.3097, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242,  0.2758,  1.7650,  2.4524,\n",
       "           2.7960,  2.7960,  2.7960,  2.7960,  2.6815,  1.2686, -0.2842,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242,  1.3068,  2.7960,  2.7960,\n",
       "           2.7960,  2.2742,  1.2941,  1.2559, -0.2206, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0][0] # первый sample данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0][1] # его таргет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.98749968e-12, 1.60027038e-15, 1.07222369e-09, 4.17137252e-03,\n",
       "        1.75877488e-14, 9.95828584e-01, 2.46962313e-16, 4.35254517e-10,\n",
       "        4.18331177e-08, 2.83568062e-11]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([data_train[0][0]]) # конвертируем данные в формат np.array размера (batch_size, channels, height, width), так как изначально они размера (channels, height, width) и формата tensor\n",
    "pred = model.forward(data) # делаем предсказание вероятностей классов\n",
    "pred # вероятности классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.argmax() # вывод индекса самого вероятного класса (совпадает с таргетом)\n",
    "# pred.argmax(axis=-1) # array([5], dtype=int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На всём датасете"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Accuracy = \\dfrac{Правильные\\ предсказания}{Все\\ предсказания}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, eval_loader) -> float:\n",
    "    \"\"\"\n",
    "    Функция для подсчёта accuracy.\\n\n",
    "    Parameters:\n",
    "        * model: оцениваемая модель\n",
    "        * eval_loader: загрузчик данных\\n\n",
    "    Returns:\n",
    "        * float: значение accuracy модели\n",
    "    \"\"\"\n",
    "    samples_num = len(eval_loader.dataset) # число объектов в датасете\n",
    "    labels_true = [] # список под настоящие классы\n",
    "    labels_pred = [] # список под предсказаные классы\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(eval_loader): # идём по батчам, что возвращает train_loader\n",
    "        target  = target.numpy() # переводим таргеты из формата tensor в numpy.array\n",
    "        for t in target: # идём по таргетам (их сразу несколько == размер батча в eval_loader)\n",
    "            labels_true.append(t) # добавляем target в список с настоящими классами\n",
    "\n",
    "        data = data.numpy() # переводим данные из формата tensor в numpy.array\n",
    "\n",
    "        preds = model.forward(data) # вызываем forward pass модели (предсказываем)\n",
    "        preds = preds.argmax(axis=-1) # из вероятностей предсказанных классов берём номера самых вероятных (\"предсказанных\")\n",
    "        for pred in preds: # идём по предсказаниям (их сразу несколько == размер батча в eval_loader)\n",
    "            labels_pred.append(pred) # добавляем предсказанный класс в список с предсказаниями\n",
    "    \n",
    "    correct_predictions = 0 # число правильных предсказаний\n",
    "    all_predictions = samples_num # число всех предсказаний, в данном случае совпадает с размером датасета\n",
    "    for i in range(samples_num): # идём по всем sample-ам в датасете\n",
    "        if labels_pred[i] == labels_true[i]: # если предсказанный класс совпадает с правильным\n",
    "            correct_predictions += 1 # увеличиваем число совпадений\n",
    "    accuracy = correct_predictions/all_predictions # считаем accuracy\n",
    "    return accuracy # возвращаем accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.958"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(model=model, eval_loader=train_loader) # подсчёт accuracy на всём датасете"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
