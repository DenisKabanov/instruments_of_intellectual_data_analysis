{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решить задачу классификации рукописных цифр на датасете mnist https://www.kaggle.com/datasets/hojjatk/mnist-dataset. Правила следующие:\n",
    "- нужно представить решение в виде нейронной сети, написанной на numpy, и обученной с помощью алгоритма градиентного спуска;\n",
    "- нейронная сеть должна состоять из двух линейных слоев, активаций relu и softmax, и mse лосса;\n",
    "- нельзя пользоваться автоградиентом (pytorch, numpy). Градиенты должны считаться вручную по алгоритму обратного распространения ошибки, используя аналитические формулы производных;\n",
    "- решение считается валидным, если оно достигает аккураси больше 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройки/Гиперпараметры/Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # для вывода графиков/картинок\n",
    "import numpy as np # для работы с матрицами\n",
    "\n",
    "import torchvision # для работы с картинками (преобразований)\n",
    "import torch # для создания модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка и обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# преобразования над датасетом\n",
    "transforms = torchvision.transforms.Compose([ # Compose объединяет несколько преобразований вместе, чтобы они выполнялись \"последовательно\"\n",
    "    torchvision.transforms.ToTensor(), # преобразование PIL изображения (или ndarray формата (Height x Width x Channels)) в tensor (типа float со значениями в области [0.0, 1.0], если такая трансформация поддерживается (см описание ToTensor))\n",
    "    torchvision.transforms.Normalize(mean=(0.1307,), std=(0.3081,)) # нормализация каналов (в датасете всего один канал) к указанным среднему значению и стандартному отклонению (цифры подобраны под датасет)\n",
    "])\n",
    "\n",
    "# датасет\n",
    "data_train = torchvision.datasets.MNIST(root=\"./data\", # путь, откуда брать/куда сохранять датасет\n",
    "                                        train=True, # скачиание обучающей части датасета\n",
    "                                        download=True, # скачивать ли датасет, если его нет в root пути\n",
    "                                        transform=transforms # функция, принимающая на вход PIL Image и преобразовывающая его\n",
    "                                       )\n",
    "\n",
    "# загрузчик данных для обучения\n",
    "train_loader = torch.utils.data.DataLoader(dataset=data_train, # указание датасета для DataLoader\n",
    "                                           batch_size=20, # размер батчка (число сэмплов, что будет возвращать DataLoader за раз) (градиент усредняется по батчу, ускоряется обработка датасета, но слегка замедляется обработка сэмпла)\n",
    "                                           num_workers=5, # число используемых ядер процессора для ускорения обработки данных\n",
    "                                           pin_memory=True # нужно ли заранее аллоцировать память под объект на GPU (лучше так, чем возвращать CUDA tensors при multi-process loading)\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train # данные о датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860, -0.1951,\n",
       "          -0.1951, -0.1951,  1.1795,  1.3068,  1.8032, -0.0933,  1.6887,\n",
       "           2.8215,  2.7197,  1.1923, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.0424,  0.0340,  0.7722,  1.5359,  1.7396,  2.7960,\n",
       "           2.7960,  2.7960,  2.7960,  2.7960,  2.4396,  1.7650,  2.7960,\n",
       "           2.6560,  2.0578,  0.3904, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           0.1995,  2.6051,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
       "           2.7960,  2.7960,  2.7960,  2.7706,  0.7595,  0.6195,  0.6195,\n",
       "           0.2886,  0.0722, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.1951,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
       "           2.0960,  1.8923,  2.7197,  2.6433, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242,  0.5940,  1.5614,  0.9377,  2.7960,  2.7960,  2.1851,\n",
       "          -0.2842, -0.4242,  0.1231,  1.5359, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.2460, -0.4115,  1.5359,  2.7960,  0.7213,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242,  1.3450,  2.7960,  1.9942,\n",
       "          -0.3988, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.2842,  1.9942,  2.7960,\n",
       "           0.4668, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0213,  2.6433,\n",
       "           2.4396,  1.6123,  0.9504, -0.4115, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6068,\n",
       "           2.6306,  2.7960,  2.7960,  1.0904, -0.1060, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           0.1486,  1.9432,  2.7960,  2.7960,  1.4850, -0.0806, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.2206,  0.7595,  2.7833,  2.7960,  1.9560, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242,  2.7451,  2.7960,  2.7451,  0.3904,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           0.1613,  1.2305,  1.9051,  2.7960,  2.7960,  2.2105, -0.3988,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0722,  1.4596,\n",
       "           2.4906,  2.7960,  2.7960,  2.7960,  2.7578,  1.8923, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.1187,  1.0268,  2.3887,  2.7960,\n",
       "           2.7960,  2.7960,  2.7960,  2.1342,  0.5686, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.1315,  0.4159,  2.2869,  2.7960,  2.7960,  2.7960,\n",
       "           2.7960,  2.0960,  0.6068, -0.3988, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1951,\n",
       "           1.7523,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.0578,\n",
       "           0.5940, -0.3097, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242,  0.2758,  1.7650,  2.4524,\n",
       "           2.7960,  2.7960,  2.7960,  2.7960,  2.6815,  1.2686, -0.2842,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242,  1.3068,  2.7960,  2.7960,\n",
       "           2.7960,  2.2742,  1.2941,  1.2559, -0.2206, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0][0] # первое \"изображение\" датасета после применения трансформаций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0][1] # класс (target) первого изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAAsTAAALEwEAmpwYAAAN8klEQVR4nO3df6jVdZ7H8ddrbfojxzI39iZOrWOEUdE6i9nSyjYRTj8o7FYMIzQ0JDl/JDSwyIb7xxSLIVu6rBSDDtXYMus0UJHFMNVm5S6BdDMrs21qoxjlphtmmv1a9b1/3K9xp+75nOs53/PD+34+4HDO+b7P93zffPHl99f53o8jQgAmvj/rdQMAuoOwA0kQdiAJwg4kQdiBJE7o5sJsc+of6LCI8FjT29qy277C9lu237F9ezvfBaCz3Op1dtuTJP1B0gJJOyW9JGlRROwozMOWHeiwTmzZ50l6JyLejYgvJf1G0sI2vg9AB7UT9hmS/jjq/c5q2p+wvcT2kO2hNpYFoE0dP0EXEeskrZPYjQd6qZ0t+y5JZ4x6/51qGoA+1E7YX5J0tu3v2j5R0o8kbaynLQB1a3k3PiIO2V4q6SlJkyQ9EBFv1NYZgFq1fOmtpYVxzA50XEd+VAPg+EHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEi0P2Yzjw6RJk4r1U045paPLX7p0acPaSSedVJx39uzZxfqtt95arN9zzz0Na4sWLSrO+/nnnxfrK1euLNbvvPPOYr0X2gq77fckHZB0WNKhiJhbR1MA6lfHlv3SiPiwhu8B0EEcswNJtBv2kPS07ZdtLxnrA7aX2B6yPdTmsgC0od3d+PkRscv2X0h6xvZ/R8Tm0R+IiHWS1kmS7WhzeQBa1NaWPSJ2Vc97JD0maV4dTQGoX8thtz3Z9pSjryX9QNL2uhoDUK92duMHJD1m++j3/HtE/L6WriaYM888s1g/8cQTi/WLL764WJ8/f37D2tSpU4vzXn/99cV6L+3cubNYX7NmTbE+ODjYsHbgwIHivK+++mqx/sILLxTr/ajlsEfEu5L+qsZeAHQQl96AJAg7kARhB5Ig7EAShB1IwhHd+1HbRP0F3Zw5c4r1TZs2Feudvs20Xx05cqRYv/nmm4v1Tz75pOVlDw8PF+sfffRRsf7WW2+1vOxOiwiPNZ0tO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwXX2GkybNq1Y37JlS7E+a9asOtupVbPe9+3bV6xfeumlDWtffvllcd6svz9oF9fZgeQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJhmyuwd69e4v1ZcuWFetXX311sf7KK68U683+pHLJtm3bivUFCxYU6wcPHizWzzvvvIa12267rTgv6sWWHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeS4H72PnDyyScX682GF167dm3D2uLFi4vz3njjjcX6hg0binX0n5bvZ7f9gO09trePmjbN9jO2366eT62zWQD1G89u/K8kXfG1abdLejYizpb0bPUeQB9rGvaI2Czp678HXShpffV6vaRr620LQN1a/W38QEQcHSzrA0kDjT5oe4mkJS0uB0BN2r4RJiKidOItItZJWidxgg7opVYvve22PV2Squc99bUEoBNaDftGSTdVr2+S9Hg97QDolKa78bY3SPq+pNNs75T0c0krJf3W9mJJ70v6YSebnOj279/f1vwff/xxy/PecsstxfrDDz9crDcbYx39o2nYI2JRg9JlNfcCoIP4uSyQBGEHkiDsQBKEHUiCsANJcIvrBDB58uSGtSeeeKI47yWXXFKsX3nllcX6008/Xayj+xiyGUiOsANJEHYgCcIOJEHYgSQIO5AEYQeS4Dr7BHfWWWcV61u3bi3W9+3bV6w/99xzxfrQ0FDD2n333Vect5v/NicSrrMDyRF2IAnCDiRB2IEkCDuQBGEHkiDsQBJcZ09ucHCwWH/wwQeL9SlTprS87OXLlxfrDz30ULE+PDxcrGfFdXYgOcIOJEHYgSQIO5AEYQeSIOxAEoQdSILr7Cg6//zzi/XVq1cX65dd1vpgv2vXri3WV6xYUazv2rWr5WUfz1q+zm77Adt7bG8fNe0O27tsb6seV9XZLID6jWc3/leSrhhj+r9ExJzq8bt62wJQt6Zhj4jNkvZ2oRcAHdTOCbqltl+rdvNPbfQh20tsD9lu/MfIAHRcq2H/haSzJM2RNCxpVaMPRsS6iJgbEXNbXBaAGrQU9ojYHRGHI+KIpF9KmldvWwDq1lLYbU8f9XZQ0vZGnwXQH5peZ7e9QdL3JZ0mabekn1fv50gKSe9J+mlENL25mOvsE8/UqVOL9WuuuaZhrdm98vaYl4u/smnTpmJ9wYIFxfpE1eg6+wnjmHHRGJPvb7sjAF3Fz2WBJAg7kARhB5Ig7EAShB1Igltc0TNffPFFsX7CCeWLRYcOHSrWL7/88oa1559/vjjv8Yw/JQ0kR9iBJAg7kARhB5Ig7EAShB1IgrADSTS96w25XXDBBcX6DTfcUKxfeOGFDWvNrqM3s2PHjmJ98+bNbX3/RMOWHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeS4Dr7BDd79uxifenSpcX6ddddV6yffvrpx9zTeB0+fLhYHx4u//XyI0eO1NnOcY8tO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwXX240Cza9mLFo010O6IZtfRZ86c2UpLtRgaGirWV6xYUaxv3LixznYmvKZbdttn2H7O9g7bb9i+rZo+zfYztt+unk/tfLsAWjWe3fhDkv4+Is6V9DeSbrV9rqTbJT0bEWdLerZ6D6BPNQ17RAxHxNbq9QFJb0qaIWmhpPXVx9ZLurZDPQKowTEds9ueKel7krZIGoiIoz9O/kDSQIN5lkha0kaPAGow7rPxtr8t6RFJP4uI/aNrMTI65JiDNkbEuoiYGxFz2+oUQFvGFXbb39JI0H8dEY9Wk3fbnl7Vp0va05kWAdSh6W68bUu6X9KbEbF6VGmjpJskrayeH+9IhxPAwMCYRzhfOffcc4v1e++9t1g/55xzjrmnumzZsqVYv/vuuxvWHn+8/E+GW1TrNZ5j9r+V9GNJr9veVk1brpGQ/9b2YknvS/phRzoEUIumYY+I/5I05uDuki6rtx0AncLPZYEkCDuQBGEHkiDsQBKEHUiCW1zHadq0aQ1ra9euLc47Z86cYn3WrFmttFSLF198sVhftWpVsf7UU08V65999tkx94TOYMsOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0mkuc5+0UUXFevLli0r1ufNm9ewNmPGjJZ6qsunn37asLZmzZrivHfddVexfvDgwZZ6Qv9hyw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSaS5zj44ONhWvR07duwo1p988sli/dChQ8V66Z7zffv2FedFHmzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJR0T5A/YZkh6SNCApJK2LiH+1fYekWyT9b/XR5RHxuybfVV4YgLZFxJijLo8n7NMlTY+IrbanSHpZ0rUaGY/9k4i4Z7xNEHag8xqFfTzjsw9LGq5eH7D9pqTe/mkWAMfsmI7Zbc+U9D1JW6pJS22/ZvsB26c2mGeJ7SHbQ+21CqAdTXfjv/qg/W1JL0haERGP2h6Q9KFGjuP/SSO7+jc3+Q5244EOa/mYXZJsf0vSk5KeiojVY9RnSnoyIs5v8j2EHeiwRmFvuhtv25Lul/Tm6KBXJ+6OGpS0vd0mAXTOeM7Gz5f0n5Jel3Skmrxc0iJJczSyG/+epJ9WJ/NK38WWHeiwtnbj60LYgc5reTcewMRA2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSKLbQzZ/KOn9Ue9Pq6b1o37trV/7kuitVXX29peNCl29n/0bC7eHImJuzxoo6Nfe+rUvid5a1a3e2I0HkiDsQBK9Dvu6Hi+/pF9769e+JHprVVd66+kxO4Du6fWWHUCXEHYgiZ6E3fYVtt+y/Y7t23vRQyO237P9uu1tvR6frhpDb4/t7aOmTbP9jO23q+cxx9jrUW932N5Vrbtttq/qUW9n2H7O9g7bb9i+rZre03VX6Ksr663rx+y2J0n6g6QFknZKeknSoojY0dVGGrD9nqS5EdHzH2DY/jtJn0h66OjQWrb/WdLeiFhZ/Ud5akT8Q5/0doeOcRjvDvXWaJjxn6iH667O4c9b0Yst+zxJ70TEuxHxpaTfSFrYgz76XkRslrT3a5MXSlpfvV6vkX8sXdegt74QEcMRsbV6fUDS0WHGe7ruCn11RS/CPkPSH0e936n+Gu89JD1t+2XbS3rdzBgGRg2z9YGkgV42M4amw3h309eGGe+bddfK8Oft4gTdN82PiL+WdKWkW6vd1b4UI8dg/XTt9BeSztLIGIDDklb1splqmPFHJP0sIvaPrvVy3Y3RV1fWWy/CvkvSGaPef6ea1hciYlf1vEfSYxo57Ognu4+OoFs97+lxP1+JiN0RcTgijkj6pXq47qphxh+R9OuIeLSa3PN1N1Zf3VpvvQj7S5LOtv1d2ydK+pGkjT3o4xtsT65OnMj2ZEk/UP8NRb1R0k3V65skPd7DXv5Evwzj3WiYcfV43fV8+POI6PpD0lUaOSP/P5L+sRc9NOhrlqRXq8cbve5N0gaN7Nb9n0bObSyW9OeSnpX0tqT/kDStj3r7N40M7f2aRoI1vUe9zdfILvprkrZVj6t6ve4KfXVlvfFzWSAJTtABSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBL/DyJ7caZa7LphAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data_train[0][0].reshape((28, 28, 1)), cmap='gray') # построение изображения\n",
    "# .reshape((28, 28, 1)), так как shape(1, 28, 28) ~ (Channels, Height, Width) не воспринимается\n",
    "# cmap='gray' так как изображение одноканальное, то есть чёрно-белое\n",
    "plt.show() # вывод изображения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Cross-Entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Cross Entropy Loss(p^{pred}) = \\dfrac{1}{batch\\_size} \\sum_{N=1}^{batch\\_size} \\sum_{C=1}^{classes} -p^{true}_{N,C}*log_e(p^{pred}_{N,C})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\dfrac{dCross Entropy Loss(p^{pred})}{dp^{pred}_{C}} = \\dfrac{1}{batch\\_size} \\sum_{N=1}^{batch\\_size} -\\dfrac{p^{true}_{N,C}}{p^{pred}_{N,C}} + \\dfrac{1-p^{true}_{N,C}}{1-p^{pred}_{N,C}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Без усреднения по батчу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Cross Entropy Loss(p^{pred}_N) = \\sum_{C=1}^{classes} -p^{true}_{N,C}*log_e(p^{pred}_{N,C}),\\ где\\ N\\ -\\ номер\\ батча\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\dfrac{dCross Entropy Loss(p^{pred}_N)}{dp^{pred}_{N,C}} = -\\dfrac{p^{true}_{N,C}}{p^{pred}_{N,C}} + \\dfrac{1-p^{true}_{N,C}}{1-p^{pred}_{N,C}},\\ где\\ N\\ -\\ номер\\ батча\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Конструктор функции для подсчёта CrossEntropyLoss.\n",
    "        \"\"\"\n",
    "        self.loss = None # значение подсчитанного loss\n",
    "        self.grad = None # градиент размера (batch_size, classes)\n",
    "        self.p_pred = None # \n",
    "        self.p_true = None\n",
    "        self.batch_size = 1 # размер батча\n",
    "        self.classes = 1 # число классов\n",
    "\n",
    "    def calc_loss(self, p_pred, p_true) -> np.float64: # результат разниться с torch.nn.CrossEntropyLoss()!\n",
    "        \"\"\"\n",
    "        Функция для подсчёта Cross-Entropy loss с усреднением по батчу.\\n\n",
    "        Parameters:\n",
    "            * p_pred: предсказанные вероятности классов размера (batch_size, classes)\n",
    "            * p_true: реальные вероятности классов размера (batch_size, classes)\\n\n",
    "        Returns:\n",
    "            * np.float64: значение функции потерь\n",
    "        \"\"\"\n",
    "        self.batch_size = p_true.shape[0] # размер батча\n",
    "        self.classes = p_true.shape[1] # число классов\n",
    "        loss = 0.0 # значение loss\n",
    "\n",
    "        # workaround для того, чтобы избавиться от inf и nan\n",
    "        p_pred[p_pred==0.0] = 0.000001 # заменяем полностью нулевые вероятности на очень малые - чтобы логарифм в формуле не давал -inf\n",
    "        p_pred[p_pred==1.0] = 0.999999 # заменяем вероятности в 1 на очень высокие - чтобы в backward в формуле (1-self.p_true[batch][c])/(1-self.p_pred[batch][c])  не получился NaN\n",
    "        p_true[p_true==0.0] = 0.000001 # заменяем полностью нулевые вероятности на очень малые - чтобы логарифм в формуле не давал -inf\n",
    "        p_true[p_true==1.0] = 0.999999 # заменяем вероятности в 1 на очень высокие - чтобы в backward в формуле (1-self.p_true[batch][c])/(1-self.p_pred[batch][c])  не получился NaN\n",
    "\n",
    "        for batch in range(self.batch_size): # идём по числу батчей (внешний цикл)\n",
    "            #========== v1\n",
    "            loss += np.matmul(p_true[batch], np.log(p_pred[batch])) # сумма по классам на определённом батче\n",
    "            #========== v2\n",
    "            # for c in range(classes):\n",
    "            #     loss += p_true[batch][c] * np.log(p_pred[batch][c])\n",
    "            #==========\n",
    "        loss = - loss / self.batch_size # домножаем на -1 и берём среднее по батчам\n",
    "\n",
    "        self.loss = loss # запоминаем подсчитанный loss\n",
    "\n",
    "        #========== v1 (без усреднения градиента по батчам)\n",
    "        self.grad = np.zeros(shape=(self.batch_size, self.classes)) # заготовка под матрицу градиентов\n",
    "        #========== v2 (с усреднением градиента по батчам)\n",
    "        # self.grad = np.zeros(shape=(self.classes)) # заготовка под матрицу градиентов (с усреднением по батчам)\n",
    "        #==========\n",
    "\n",
    "        self.p_pred = p_pred\n",
    "        self.p_true = p_true\n",
    "        return loss # возвращаем посчитанный loss\n",
    "    \n",
    "\n",
    "    def backward(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Функция для подсчёта градиента после Cross-Entropy loss.\\n\n",
    "        Returns:\n",
    "            * np.ndarray: значение градиента размер (batch_size, classes)\n",
    "        \"\"\"\n",
    "        #========== v1 (если считать, что вход пришёл от softmax, то сразу возвращаем dloss/dsoftmax_input)\n",
    "        # for batch in range(self.batch_size): # идём по номерам батчей\n",
    "        #     #========== v1 с итерированием по классам\n",
    "        #     for c in range(self.classes): # идём по классам\n",
    "        #         self.grad[batch][c] = self.p_pred[batch][c] - self.p_true[batch][c] # считаем градиент при условии, что вход от softmax (см grad.png)\n",
    "        #     #========== v2 без итерирования по классам (работаем с векторами-строками)\n",
    "        #     self.grad[batch] = self.p_pred[batch] - self.p_true[batch] # считаем градиент при условии, что вход от softmax\n",
    "        #     #==========\n",
    "        #========== v1.1 (v1 с усреднением по батчам)\n",
    "        # for batch in range(self.batch_size): # идём по номерам батчей\n",
    "        #     #========== v1 с итерированием по классам\n",
    "        #     # for c in range(self.classes): # идём по классам\n",
    "        #     #     self.grad[c] += self.p_pred[batch][c] - self.p_true[batch][c] # считаем градиент (см grad.png)\n",
    "        #     #========== v2 без итерирования по классам (работаем с векторами-строками)\n",
    "        #     self.grad += self.p_pred[batch] - self.p_true[batch] # считаем градиент\n",
    "        #     #==========\n",
    "        # self.grad = self.grad / self.batch_size # усредняем градиент по числу батчей\n",
    "        #========== v2 (общий случай)\n",
    "        for batch in range(self.batch_size): # идём по номерам батчей\n",
    "            #========== v1 с итерированием по классам\n",
    "            # for c in range(self.classes): # идём по классам\n",
    "            #     self.grad[batch][c] = -self.p_true[batch][c]/self.p_pred[batch][c] + (1-self.p_true[batch][c])/(1-self.p_pred[batch][c])\n",
    "            #========== v2 без итерирования по классам (работаем с векторами-строками)\n",
    "            self.grad[batch] = -(self.p_true[batch]/self.p_pred[batch]) + (1-self.p_true[batch])/(1-self.p_pred[batch]) # считаем градиент от предсказанных вероятностей\n",
    "            #==========\n",
    "        #========== v2.1 (v2 с усреднением по батчам)\n",
    "        # for batch in range(self.batch_size): # идём по номерам батчей\n",
    "        #     #========== v1 с итерированием по классам\n",
    "        #     # for c in range(self.classes): # идём по классам\n",
    "        #     #     self.grad[c] += -(self.p_true[batch][c]/self.p_pred[batch][c]) + (1-self.p_true[batch][c])/(1-self.p_pred[batch][c]) # считаем градиент от предсказанных вероятностей\n",
    "        #     #========== v2 без итерирования по классам (работаем с векторами-строками)\n",
    "        #     self.grad += -(self.p_true[batch]/self.p_pred[batch]) + (1-self.p_true[batch])/(1-self.p_pred[batch]) # считаем градиент от предсказанных вероятностей\n",
    "        #     #==========\n",
    "        # self.grad = self.grad / self.batch_size # усредняем градиент по числу батчей\n",
    "        #==========\n",
    "\n",
    "        return self.grad # возвращаем посчитанный градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9009, dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_pred = np.array([[0.2,0.6,0.2], [1.0,0.0,0.0]])\n",
    "# probs_pred = np.array([[1.0,0.0,0.0], [1.0,0.0,0.0]])\n",
    "probs_pred = torch.tensor(probs_pred)\n",
    "\n",
    "probs_true = np.array([[1.0,0.0,0.0], [1.0,0.0,0.0]])\n",
    "probs_true = torch.tensor(probs_true)\n",
    "\n",
    "loss_torch = torch.nn.CrossEntropyLoss()\n",
    "loss_torch(probs_pred, probs_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8047, dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_custom = CrossEntropyLoss()\n",
    "loss_custom.calc_loss(probs_pred, probs_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.99999375,  2.49999583,  1.24999375],\n",
       "       [ 0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_custom.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "ReLU(x) =  \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      0 & if & x \\leq 0 \\\\\n",
    "      x & if & x > 0 \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\dfrac{dReLU(x)}{dx} =  \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      0 & if & x \\leq 0 \\\\\n",
    "      1 & if & x > 0 \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"pics/ReLU.png\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Конструктор ReLU функции.\n",
    "        \"\"\"\n",
    "        self.prev_input = None # вход с предыдущего слоя\n",
    "\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray: # numpy.array — это просто удобная функция для создания numpy.ndarray\n",
    "        \"\"\"\n",
    "        Функция активации ReLU, если значение в X меньше или равно нулю - оно становится нулём, иначе — остаётся прежним.\\n\n",
    "        Parameters:\n",
    "            * x: данные в виде массива размера (batch_size, features)\\n\n",
    "        Returns:\n",
    "            * np.ndarray: преобразованный x согласно работе функции активации\n",
    "        \"\"\"\n",
    "        self.prev_input = x.copy() # запоминаем вход с предыдущего шага\n",
    "        x[x<0] = 0.0 # заменяем все значения в x, что меньше нуля на ноль\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Функция для обратного прохода градиента.\\n\n",
    "        Parameters:\n",
    "            * grad: градиент, пришедший со следующего шага\\n\n",
    "        Returns:\n",
    "            * np.ndarray: градиент с учётом текущего шага, передающийся назад\n",
    "        \"\"\"\n",
    "        grad[self.prev_input < 0] = 0 # зануляем градиент там, где вход был меньше нуля\n",
    "        # как бы поэлементно умножаем пришедший градиент на матрицу из нулей и единиц (так как это производная ReLU) \n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Softmax(x) = \\dfrac{e^{x_i}}{\\sum_{j}e^{x_j}} = p_i,\\ где\\ i=\\overline{1, N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\dfrac{dSoftmax(x)}{dx_i} = \\dfrac{e^{x_i}}{\\sum_{j}e^{x_j}} (1 - \\dfrac{e^{x_i}}{\\sum_{j}e^{x_j}}) = p_i * (1 - p_i),\\ где\\ i=\\overline{1, N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Конструктор Softmax функции.\n",
    "        \"\"\"\n",
    "        # print(\"constructed\")\n",
    "        self.prev_input = None # вход с предыдущего слоя\n",
    "        self.prev_out = None # выход со слоя\n",
    "\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Функция Softmax, пересчитывающая для всех элементов массива x значение e^x_i/summ(e^x_i).\\n\n",
    "        Parameters:\n",
    "            * x: данные в виде массива размерности (batch_size, class_count) ~ (размер батча, число классов)\\n\n",
    "        Returns:\n",
    "            * np.ndarray: преобразованный X согласно формуле Softmax\n",
    "        \"\"\"\n",
    "        prob = np.exp(x) # считаем экспоненту от всех элементов X (каждый элемент в X идёт как степень e)\n",
    "\n",
    "        for batch in range(prob.shape[0]): # идём по батчам\n",
    "            prob[batch] = prob[batch] / prob[batch].sum() # каждый элемент (экспоненту) в батче делим на сумму экспонент этого батча\n",
    "        self.prev_out = prob.copy() # запоминаем выход слоя\n",
    "        # print(\"prob\\n\", prob) # DEBUG\n",
    "        return prob # возвращаем результат Softmax\n",
    "    \n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray: # grad = dloss/dinput = dloss/dprob * dprob/dinput, где prob - выход слоя softmax\n",
    "        \"\"\"\n",
    "        Функция для обратного прохода градиента.\\n\n",
    "        Parameters:\n",
    "            * grad: градиент, пришедший со следующего шага\\n\n",
    "        Returns:\n",
    "            * np.ndarray: градиент с учётом текущего шага, передающийся назад\n",
    "        \"\"\"\n",
    "        #========== v1 (если в CrossEntropyLoss учитывается то, что вход от softmax ==> грубо говоря - пропускаем шаг с подсчётом dloss/dprob, а сразу считаем dloss/dinput)\n",
    "        # return grad # просто возвращаем полученный градиент (при условии формулы в CrossEntropyLoss)\n",
    "        #========== v2 (общий случай)\n",
    "        return grad * self.prev_out * (1 - self.prev_out) # домножаем полученный градиент на значение производной (dot product - поэлементное умножение)\n",
    "        #=========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.99833498e-01 6.00333004e-01 1.99833498e-01]\n",
      " [9.99909208e-01 4.53958078e-05 4.53958078e-05]]\n",
      "[[-8.00165502e-01  6.00332004e-01  1.99832498e-01]\n",
      " [-8.97916157e-05  4.43958078e-05  4.43958078e-05]]\n"
     ]
    }
   ],
   "source": [
    "# probs_pred = np.array([[0.2,0.6,0.2], [1.0,0.0,0.0]])\n",
    "# probs_pred = np.array([[1.0,0.0,0.0], [1.0,0.0,0.0]])\n",
    "input = np.array([[2.0,3.1,2.0], [10.0,0.0,0.0]]) # ==> probs_pred ~ [[0.2,0.6,0.2], [1.0,0.0,0.0]]\n",
    "\n",
    "probs_true = np.array([[1.0,0.0,0.0], [1.0,0.0,0.0]])\n",
    "\n",
    "soft = Softmax()\n",
    "res = soft.forward(input)\n",
    "print(res)\n",
    "\n",
    "loss_custom = CrossEntropyLoss()\n",
    "loss_custom.calc_loss(res, np.array([[1.0,0.0,0.0], [1.0,0.0,0.0]]))\n",
    "grad = loss_custom.backward()\n",
    "print(soft.backward(grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Linear(x) = xW + b\n",
    "$$\n",
    "* x - вход размера (batch_size, in_features)\n",
    "* W - матрица весов размера (in_features, out_features)\n",
    "* b - вектор-строка для смещения размера (out_features)\n",
    "* y - выход размера (batch_size, out_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\dfrac{dLinear(x)}{dx} = W^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\dfrac{dLinear(x)}{dW} = x^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\dfrac{dLinear(x)}{db} = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear: # линейный слой без смещения (bias)\n",
    "    def __init__(self, in_features, out_features):\n",
    "        \"\"\"\n",
    "        Конструктор линейного слоя.\n",
    "        \"\"\"\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.prev_input = None # вход с предыдущего слоя\n",
    "\n",
    "        # задаём начальные данные для модели из равномерного распределения от -0.5 до 0.5\n",
    "        self.W = np.random.uniform(low=-0.5, high=0.5, size=(in_features, out_features)) # матрица весов слоя\n",
    "        self.bias = np.random.uniform(low=-0.5, high=0.5, size=(out_features)) # смещение как вектор-строка\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Функция, применяющая веса и смещение к входным данным, чья размерность (batch_size, in_features).\\n\n",
    "        \"\"\"\n",
    "        self.prev_input = x.copy() # запоминаем вход с предыдущего шага\n",
    "\n",
    "        #========== v1 (умножение на веса)\n",
    "        res = np.matmul(x, self.W) # умножаем вход на веса (batch_size, in_features)x(in_features, out_features)=(batch_size, out_features)\n",
    "        #========== v1.1\n",
    "        # res = np.zeros(shape=(x.shape[0], self.out_features))\n",
    "        # for batch in range(x.shape[0]):\n",
    "        #     res[batch] = np.matmul(x[batch], w)\n",
    "        #========== v1.2\n",
    "        # res = np.zeros(shape=(x.shape[0], self.out_features))\n",
    "        # for batch_id, batch in enumerate(x):\n",
    "        #     res[batch_id] = np.matmul(batch, w)\n",
    "        #==========\n",
    "\n",
    "        #========== v1 (добавление смещения)\n",
    "        res = np.add(res, self.bias) # добавляем смещение (bias) для каждого батча\n",
    "        #========== v1.1\n",
    "        # for batch in res:\n",
    "        #     batch += self.bias\n",
    "        #==========\n",
    "        return res\n",
    "        \n",
    "    \n",
    "    def backward(self, grad: np.ndarray, lr: np.float64=0.0001):\n",
    "        \"\"\"\n",
    "        Функция для обратного прохода градиента (обновляющая веса и смещение).\\n\n",
    "        Parameters:\n",
    "            * grad: градиент, пришедший со следующего шага\\n\n",
    "        Returns:\n",
    "            * np.ndarray: градиент с учётом текущего шага, передающийся назад\n",
    "        \"\"\"\n",
    "        # print(\"grad from next step\", grad.shape) # (20, 10)\n",
    "        # print(\"prev_input\", self.prev_input.shape) # (20, 196)\n",
    "        # print(\"prev_input.T\", self.prev_input.T.shape) # (196, 20)\n",
    "        # grad_W = np.matmul(grad, self.prev_input.T) # считаем градиент ошибки по весу W\n",
    "        grad_W = np.matmul(self.prev_input.T, grad) # считаем градиент ошибки по весу W\n",
    "\n",
    "        # print(\"current W\", self.W.shape) # (196, 10)\n",
    "        # print(\"grad_W\", grad_W.shape) # (196, 10)\n",
    "        # self.W = self.W - lr * grad_W.sum(axis=0)/grad_W.shape[0] # обновляем веса (усредняя градиент по батчам)\n",
    "        self.W = self.W - lr * grad_W # обновляем веса\n",
    "\n",
    "        grad_b = grad # считаем градиент ошибки по смещению\n",
    "        self.bias = self.bias - lr * grad_b.sum(axis=0)/grad_b.shape[0] # обновляем смещение (усредняя градиент по батчам)\n",
    "        # self.bias = self.bias - lr * grad_b\n",
    "        \n",
    "        grad = np.matmul(grad, self.W.T) # градиент для предыдущего слоя\n",
    "\n",
    "        return grad "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример данных для модели\n",
    "x = torch.randn(size=(10, 5)) # вход\n",
    "w = torch.randn(size=(5, 3)) # вес\n",
    "b = torch.randn(3) # смещение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.40002194,  3.72322685,  0.40824163],\n",
       "       [-0.79332903,  2.9594155 , -1.09498012],\n",
       "       [ 0.65042424,  0.94367045,  0.35207057],\n",
       "       [ 3.19763038,  1.75812763, -0.09431708],\n",
       "       [-1.57512549, -0.4743821 ,  4.38424957],\n",
       "       [-1.87161806,  3.35738999, -1.30589068],\n",
       "       [-2.19743207, -1.08883315, -0.24993384],\n",
       "       [ 2.70750448, -4.24223179,  2.55342877],\n",
       "       [ 0.86797532,  1.01374426, -0.6921711 ],\n",
       "       [-0.78623191,  0.98088628,  0.16687959]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# вариант по батчам v1\n",
    "res = np.zeros(shape=(10,3))\n",
    "for batch in range(10):\n",
    "    res[batch] = np.matmul(x[batch], w)\n",
    "    res[batch] = np.add(res[batch], b)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.400022  ,  3.7232268 ,  0.40824163],\n",
       "       [-0.7933289 ,  2.9594154 , -1.0949801 ],\n",
       "       [ 0.65042424,  0.94367045,  0.35207057],\n",
       "       [ 3.1976304 ,  1.7581277 , -0.09431708],\n",
       "       [-1.5751255 , -0.4743821 ,  4.3842497 ],\n",
       "       [-1.871618  ,  3.35739   , -1.3058907 ],\n",
       "       [-2.197432  , -1.0888331 , -0.24993384],\n",
       "       [ 2.7075043 , -4.242232  ,  2.5534286 ],\n",
       "       [ 0.86797535,  1.0137442 , -0.69217134],\n",
       "       [-0.7862319 ,  0.9808863 ,  0.1668796 ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = Linear(5, 3)\n",
    "linear.W = w.numpy()\n",
    "linear.bias = b.numpy()\n",
    "linear.forward(x.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4000,  3.7232,  0.4082],\n",
       "        [-0.7933,  2.9594, -1.0950],\n",
       "        [ 0.6504,  0.9437,  0.3521],\n",
       "        [ 3.1976,  1.7581, -0.0943],\n",
       "        [-1.5751, -0.4744,  4.3842],\n",
       "        [-1.8716,  3.3574, -1.3059],\n",
       "        [-2.1974, -1.0888, -0.2499],\n",
       "        [ 2.7075, -4.2422,  2.5534],\n",
       "        [ 0.8680,  1.0137, -0.6922],\n",
       "        [-0.7862,  0.9809,  0.1669]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# вариант по батчам v2\n",
    "res = np.matmul(x, w)\n",
    "for batch in res:\n",
    "    batch += b\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4000,  3.7232,  0.4082],\n",
       "        [-0.7933,  2.9594, -1.0950],\n",
       "        [ 0.6504,  0.9437,  0.3521],\n",
       "        [ 3.1976,  1.7581, -0.0943],\n",
       "        [-1.5751, -0.4744,  4.3842],\n",
       "        [-1.8716,  3.3574, -1.3059],\n",
       "        [-2.1974, -1.0888, -0.2499],\n",
       "        [ 2.7075, -4.2422,  2.5534],\n",
       "        [ 0.8680,  1.0137, -0.6922],\n",
       "        [-0.7862,  0.9809,  0.1669]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# однострочный вариант\n",
    "np.add(np.matmul(x, w), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNetwork():\n",
    "    def __init__(self, in_features, out_features):\n",
    "        # 196 = 784/4, то есть уменьшили в 4 раза\n",
    "        self.linear1 = Linear(in_features, int(in_features/4)) # задаём первый линейный слой\n",
    "        self.activation = ReLU() # задаём активацию\n",
    "        self.linear2 = Linear(int(in_features/4), 10) # задаём второй линейный слой\n",
    "        self.softmax = Softmax() # задаём softmax\n",
    "        self.layers = [] # список всех слоёв модели\n",
    "        self.layers.append(self.linear1)\n",
    "        self.layers.append(self.activation)\n",
    "        self.layers.append(self.linear2)\n",
    "        self.layers.append(self.softmax)\n",
    "\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Функция для вызова forward метода всех слоёв модели.\\n\n",
    "        Parameters:\n",
    "            * x: данные на вход размера (batch_size, channels, height, width)\\n\n",
    "        Returns:\n",
    "            * np.ndarray: результат вызова всех слоёв модели\n",
    "        \"\"\"\n",
    "        x = x.reshape(x.shape[0], x.shape[1]*x.shape[2]*x.shape[3]) # \"избавляемся\" от размерностей channels, height, width, совмещаем их в одномерный массив -> получаем двумерный массив размера (batch_size, channels*height*width)\n",
    "        for layer in self.layers: # идём по слоям модели\n",
    "            x = layer.forward(x) # последовательно вызываем forward метод каждого слоя\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def backward(self, grad) -> None:\n",
    "        \"\"\"\n",
    "        Функция для обновления весов модели.\\n\n",
    "        Parameters:\n",
    "            * grad: значение градиента, используемое для обновления весов\\n\n",
    "        Returns:\n",
    "            * None: обновляет веса модели путём градиентного спуска\n",
    "        \"\"\"\n",
    "        for layer in reversed(self.layers): # идём по слоям в обратном порядке\n",
    "            grad = layer.backward(grad) # обновляем веса у слоя и возвращаем изменённый loss \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs: np.int64, train_loader, loss_func, num_classes: np.int64) -> list: # функция обучения\n",
    "    \"\"\"\n",
    "    Функция обучения модели.\\n\n",
    "    Parameters:\n",
    "        * model: модель для обучения\n",
    "        * epochs: число эпох обучения\n",
    "        * train_loader: загрузчик данных\n",
    "        * loss_func: функция для подсчёта потерь\n",
    "        * num_classes: число классов в задаче multi-class classification\\n\n",
    "    Returns:\n",
    "        * list: список с получившимися значениями loss функции\n",
    "    \"\"\"\n",
    "    losses = [0.0] * epochs # заготавливаем массив под значения loss функции\n",
    "    for epoch in range(epochs): # обучаемся по эпохам\n",
    "        for batch_idx, (data, target) in enumerate(train_loader): # идём по батчам, что возвращает train_loader\n",
    "            # переводим target в вероятностное пространство (везде нули, кроме нужного таргета - там единица)\n",
    "            # p_true = np.array([0.0 if i != target else 1.0 for i in range(num_classes)], dtype=np.float64) # переводим target в вероятностное пространство (везде нули, кроме нужного таргета - там единица)\n",
    "            p_true = np.zeros(shape=(train_loader.batch_size, num_classes)) # заготовка под вероятности (пока заполнена нулями)\n",
    "            for batch, t in enumerate(target): # идём по батчу (нескольким сэмплам), t - id правильного таргета\n",
    "                p_true[batch][t] = 1.0 # ставим вероятность 1 у нужных таргетов (для всех элементов в батче)\n",
    "\n",
    "            data = data.numpy() # переводим данные из формата tensor в numpy.array\n",
    "\n",
    "            p_pred = model.forward(data) # вызываем forward pass модели (предсказываем)\n",
    "\n",
    "            loss = loss_func.calc_loss(p_pred, p_true) # считаем loss модели\n",
    "            losses[epoch] += loss/len(train_loader) # добавляем посчитанный на батче loss (делённый на len(train_loader) - число батчей, для усреднения)\n",
    "\n",
    "            grad = loss_func.backward() # считаем градиент ошибки\n",
    "            model.backward(grad) # обновляем веса модели\n",
    "        print(f\"loss on epoch {epoch+1}:\\t {losses[epoch]}\")\n",
    "    return losses\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 1:\t 2.537830716006109\n",
      "loss on epoch 2:\t 1.0050197685861515\n",
      "loss on epoch 3:\t 0.7345215820300226\n",
      "loss on epoch 4:\t 0.5890121267976615\n",
      "loss on epoch 5:\t 0.4949086470272617\n",
      "loss on epoch 6:\t 0.4283844776784835\n",
      "loss on epoch 7:\t 0.3785665644713339\n",
      "loss on epoch 8:\t 0.34004839090185063\n",
      "loss on epoch 9:\t 0.30920815672639096\n",
      "loss on epoch 10:\t 0.2838382520074111\n",
      "loss on epoch 11:\t 0.26256859717990166\n",
      "loss on epoch 12:\t 0.24452216416955175\n",
      "loss on epoch 13:\t 0.2290318475164834\n",
      "loss on epoch 14:\t 0.21548709648942577\n",
      "loss on epoch 15:\t 0.20355054388626165\n",
      "loss on epoch 16:\t 0.19296611844565795\n",
      "loss on epoch 17:\t 0.1834970651124477\n",
      "loss on epoch 18:\t 0.17498793156753364\n",
      "loss on epoch 19:\t 0.16729490731098362\n",
      "loss on epoch 20:\t 0.16031013180308892\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10 # число классов (различных цифр)\n",
    "epochs = 20 # число эпох обучения\n",
    "model = CustomNetwork(in_features=np.prod(data_train[0][0].shape), out_features=num_classes) # np.prod(data_train[0][0].shape) - произведение всех размерностей входных данных\n",
    "loss_func = CrossEntropyLoss()\n",
    "losses = train(model=model, epochs=epochs, train_loader=train_loader, loss_func=loss_func, num_classes=num_classes)\n",
    "# loss - кросс-энтропия (перекрёстная энтропия)\n",
    "# CrossEntropyLoss на вход ожидает вероятность класса для всех k классов\n",
    "# то есть массив с вероятностями для каждой из четырёх категорий новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10)) # размер графика\n",
    "plt.title(\"Изменение Cross-Entropy Loss в зависимости от эпохи обучения\") # название графика\n",
    "plt.plot(range(1, epochs+1), losses, label=\"Cross-Entropy Loss\") # построение графика, где range(1, epochs+1) — рассматриваемые значения x, losses — отображаемые значения, label — названия графика\n",
    "plt.xlabel(\"Эпоха\") # подпись по оси x\n",
    "plt.ylabel(\"Значение loss функции\") # подпись по оси y\n",
    "plt.legend() # вывод названий графиков\n",
    "plt.show() # вывод графика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверка работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860, -0.1951,\n",
       "          -0.1951, -0.1951,  1.1795,  1.3068,  1.8032, -0.0933,  1.6887,\n",
       "           2.8215,  2.7197,  1.1923, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.0424,  0.0340,  0.7722,  1.5359,  1.7396,  2.7960,\n",
       "           2.7960,  2.7960,  2.7960,  2.7960,  2.4396,  1.7650,  2.7960,\n",
       "           2.6560,  2.0578,  0.3904, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           0.1995,  2.6051,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
       "           2.7960,  2.7960,  2.7960,  2.7706,  0.7595,  0.6195,  0.6195,\n",
       "           0.2886,  0.0722, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.1951,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
       "           2.0960,  1.8923,  2.7197,  2.6433, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242,  0.5940,  1.5614,  0.9377,  2.7960,  2.7960,  2.1851,\n",
       "          -0.2842, -0.4242,  0.1231,  1.5359, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.2460, -0.4115,  1.5359,  2.7960,  0.7213,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242,  1.3450,  2.7960,  1.9942,\n",
       "          -0.3988, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.2842,  1.9942,  2.7960,\n",
       "           0.4668, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0213,  2.6433,\n",
       "           2.4396,  1.6123,  0.9504, -0.4115, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6068,\n",
       "           2.6306,  2.7960,  2.7960,  1.0904, -0.1060, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           0.1486,  1.9432,  2.7960,  2.7960,  1.4850, -0.0806, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.2206,  0.7595,  2.7833,  2.7960,  1.9560, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242,  2.7451,  2.7960,  2.7451,  0.3904,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           0.1613,  1.2305,  1.9051,  2.7960,  2.7960,  2.2105, -0.3988,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0722,  1.4596,\n",
       "           2.4906,  2.7960,  2.7960,  2.7960,  2.7578,  1.8923, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.1187,  1.0268,  2.3887,  2.7960,\n",
       "           2.7960,  2.7960,  2.7960,  2.1342,  0.5686, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.1315,  0.4159,  2.2869,  2.7960,  2.7960,  2.7960,\n",
       "           2.7960,  2.0960,  0.6068, -0.3988, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1951,\n",
       "           1.7523,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.0578,\n",
       "           0.5940, -0.3097, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242,  0.2758,  1.7650,  2.4524,\n",
       "           2.7960,  2.7960,  2.7960,  2.7960,  2.6815,  1.2686, -0.2842,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242,  1.3068,  2.7960,  2.7960,\n",
       "           2.7960,  2.2742,  1.2941,  1.2559, -0.2206, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0][0] # первый sample данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0][1] # его таргет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.66165886e-05, 7.65384096e-13, 2.13083707e-06, 1.32698713e-01,\n",
       "        2.02157506e-17, 8.67219171e-01, 3.53080442e-21, 4.84264099e-13,\n",
       "        4.43803039e-09, 3.36368073e-06]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([data_train[0][0]]) # конвертируем данные в формат np.array размера (batch_size, channels, height, width), так как изначально они размера (channels, height, width) и формата tensor\n",
    "pred = model.forward(data) # делаем предсказание вероятностей классов\n",
    "pred # вероятности классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.argmax() # вывод индекса самого вероятного класса (совпадает с таргетом)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
